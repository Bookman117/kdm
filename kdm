#!/bin/bash
#Purpose: Deploy kubernetes on ubuntu server 20.04/22.04 LTS & Rocky Linux 8/9 & RHEL 8/9.
#Create-Date: 2022-08-10

#Source function & variable
ls ~/bin/kdm_function &> /dev/null
[ ${?} == 0 ] && source ~/bin/kdm_function &> /dev/null
ls ~/kdm/kdm_function &> /dev/null
[ ${?} == 0 ] && source ~/kdm/kdm_function &> /dev/null

#Environments pre-setting
host-os-detection
setup-network-package
check-kdm-path

#Variables>>>
#Network variable
set-network-variable
#Kubernetes node variable
kubernetes-node-variable
#Text color
set-text-color-variable
#Kubernetes projects namespace variable
kubernetes-namespace-variable

#Program >>>
case ${1} in

sys-info) #Show host basic information.
  system-info
;;

sys-var) #Check script variables.
  show-version-control
;;

sys-conf) #Configure file & directory.
  #Set localhost hostname in hosts
  cat /etc/hosts | grep '127.0.0.1' | grep ${HOSTNAME} &> /dev/null
  [ ${?} != 0 ] &&  sudo sed -i "s|127.0.0.1   |127.0.0.1   ${HOSTNAME} |g" /etc/hosts
  echo -en " [${GREEN}●${NC}] /etc/hosts: "
  cat /etc/hosts | grep '127.0.0.1' | grep ${HOSTNAME}

  [ "${#}" == "2" ] && [ "${2}" == "hosts" ] && status=1 || status=0
  [ "${status}" == "1" ] && node-selector hosts && status=1
  
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}${install_list} system configure${NC}"
      [ "${status}" == "1" ] && ssh ${install_list} 'kdm sys-conf'
    done
  
  [ "${status}" == "1" ] && exit
  #setup ssh
  host-os-detection
  cat /etc/ssh/ssh_config | grep 'StrictHostKeyChecking no'
  [ ${?} != 0 ] && echo 'StrictHostKeyChecking no' | sudo tee -a /etc/ssh/ssh_config

  [ "${OS_VER}" == "RHEL_8" ] && sudo -S sed -i "s/# %wheel\tALL=(ALL)\tNOPASSWD: ALL/%wheel\tALL=(ALL)\tNOPASSWD: ALL/g" /etc/sudoers
  [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo -S sed -i "s/# %wheel\tALL=(ALL)\tNOPASSWD: ALL/%wheel\tALL=(ALL)\tNOPASSWD: ALL/g" /etc/sudoers
  [ "${OS_VER}" == "xUbuntu_22.04" ] && sudo -S sed -i "s/%sudo\tALL=(ALL:ALL) ALL/%sudo\tALL=(ALL:ALL) NOPASSWD: ALL/g" /etc/sudoers

  [ "${OS_VER}" == "RHEL_8" ] && sudo subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms &> /dev/null
  [ "${OS_VER}" == "RHEL_8" ] && sudo dnf update -y && sudo dnf upgrade -y
  [ "${OS_VER}" == "RHEL_8" ] && sudo dnf install -y nano tree curl git chrony
  [ "${OS_VER}" == "RHEL_8" ] && sudo systemctl start chronyd && sudo systemctl enable chronyd.service
  [ "${OS_VER}" == "RHEL_8" ] && sudo systemctl status chronyd | grep 'Active:' && sudo chronyc -a makestep
  [ "${OS_VER}" == "RHEL_8" ] && sudo timedatectl set-timezone Asia/Taipei
  
  [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo dnf update -y && sudo dnf upgrade -y
  [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo dnf install -y nano tree curl git chrony epel-release
  [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo systemctl start chronyd && sudo systemctl enable chronyd.service
  [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo systemctl status chronyd | grep 'Active:' && sudo chronyc -a makestep
  [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo timedatectl set-timezone Asia/Taipei
  #[ "${OS_VER}" == "CentOS_8_Stream" ] && sudo dnf install -y systemd-timesyncd && sudo systemctl enable --now systemd-timesyncd.service && sudo timedatectl set-ntp true && sudo timedatectl set-timezone Asia/Taipei

  [ "${OS_VER}" == "xUbuntu_22.04" ] && sudo apt -qy update && sudo apt -qy upgrade
  [ "${OS_VER}" == "xUbuntu_22.04" ] && sudo apt install -qy nano tree curl git chrony
  [ "${OS_VER}" == "xUbuntu_22.04" ] && sudo systemctl start chronyd && sudo systemctl enable chronyd.service
  [ "${OS_VER}" == "xUbuntu_22.04" ] && sudo systemctl status chronyd | grep 'Active:'&& sudo chronyc -a makestep
  [ "${OS_VER}" == "xUbuntu_22.04" ] && sudo timedatectl set-timezone Asia/Taipei

  #turn off welcome message
  cat /etc/profile | grep 'clear'
  [ ${?} != 0 ] && echo "clear" | sudo tee -a /etc/profile
  touch ~/.hushlogin
  [ "${OS_VER}" == "xUbuntu_22.04" ] && sudo chmod -x /etc/update-motd.d/*

  #Set /etc/hosts
  cat /etc/hosts | grep '127.0.0.1' | grep ${HOSTNAME} &> /dev/null
  [ ${?} != 0 ] && sudo sed -i "s|127.0.0.1   |127.0.0.1   ${HOSTNAME} |g" /etc/hosts

  echo "/etc/hosts:"
  echo -en "${GREEN}"
  cat /etc/hosts | grep '127.0.0.1' | grep ${HOSTNAME}
  echo -en "${NC}"
;;

sys-check) #Check node basic status.
  system-check ${@}
;;

sys-date) #Check system time-zone.
  node-selector ${@}
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -en "${YELLOW}${install_list} system time: ${NC}"
      ssh ${install_list} 'date +"%Y-%m-%d %Z %H:%M:%S"'
    done
;;

set-ssh-key) #Let ssh login without password. [ host | renew ]
  if [ "${2}" == "local" ]
    then
      sudo rm -r .ssh/*
      ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa <<< y
      ssh-copy-id ${USER}@localhost
    elif [ "${2}" == "renew" ]
      then
        sudo rm -r .ssh/*
        ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa <<< y
        ssh-copy-id ${USER}@localhost
        for install_list in ${ALL_NODES_EX_LOCALHOST}
          do
            scp -r .ssh ${install_list}:
          done
      else
        echo -e "${YELLOW}Please input parameter [ local | renew ]${NC}"
  fi
;;

set-hosts) #Setup hosts. [ hosts m start end w start end [ Detect NETID just input xxx xxx ] ]
  #setup /etc/hosts
  declare -i mstart=${3} mend=${4} wstart=${6} wend=${7} number=1
  m=${2} w=${5}
  if [ ${#} != 7 ]
    then
      echo -e "Please input parameter.\n"
      echo -e "hosts: setup hosts [ m start end w start end ]\n"; exit
    else

  sudo sed -i "/${NETID}/d" /etc/hosts

  for ((mstart;mstart<=mend;mstart=mstart+1))
    do
sudo bash -c "cat << EOF >> /etc/hosts
${NETID}.${mstart} ${mstart}-${m}${number}
EOF"
  number=$((number+1))
    done;

number=1

  for ((wstart;wstart<=wend;wstart=wstart+1))
    do
sudo bash -c "cat << EOF >> /etc/hosts
${NETID}.${wstart} ${wstart}-${w}${number}
EOF"
  number=$((number+1))
    done;

  echo -e "=hosts=\n"; cat /etc/hosts
  fi

  echo -e "\n= Prepare power off to duplicate VM node."
  interrupt; echo "poweroff node..."; sleep 1.5
  sudo poweroff
;;

set-ip) #Setup IP Address. [ set-ip IP/NETMASK [ Detect NETID just input xxx/XX ] ]
#Need fix input parameter.
  echo "${2}" | grep '\/' &> /dev/null
  if [ ${?} != 0 ]
    then
      echo -e "${RED}Please input parameter after ${YELLOW}\"set-ip\"${NC} [ set-ip IP/NETMASK [ Detect NETID just input xxx/XX ] ]\n" && exit
  fi
  if [ ${#} != 2 ]
    then
      echo -e "${RED}Please input parameter after ${YELLOW}\"set-ip\"${NC} [ set-ip IP/NETMASK [ Detect NETID just input xxx/XX ] ]\n" && exit
    else

#Rocky 8 NetworkManager setting
[ "${OS_VER}" == "CentOS_8_Stream" ] && sudo bash -c "cat << EOF > /etc/sysconfig/network-scripts/ifcfg-${KUBE_INTERFACE}
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=no
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
NAME=${KUBE_INTERFACE}
UUID=${NETWORK_UUID}
DEVICE=${KUBE_INTERFACE}
ONBOOT=yes
IPADDR=${NETID}.
PREFIX=24
GATEWAY=${GATEWAY}
DNS1=8.8.8.8
EOF" && echo -e "Network-scripts setting\n" && sudo cat /etc/sysconfig/network-scripts/ifcfg-${KUBE_INTERFACE}

#RHEL 9 & Rocky 9 NetworkManager setting
[ "${OS_VER}" == "CentOS_8_Stream" ] && uuid=$(sudo cat /etc/NetworkManager/system-connections/${KUBE_INTERFACE}.nmconnection | grep 'uuid' | cut -d '=' -f 2)
[ "${OS_VER}" == "CentOS_8_Stream" ] && timestamp=$(sudo cat /etc/NetworkManager/system-connections/${KUBE_INTERFACE}.nmconnection | grep 'timestamp' | cut -d '=' -f 2)
[ "${OS_VER}" == "CentOS_8_Stream" ] && sudo bash -c "cat << EOF > /etc/NetworkManager/system-connections/${KUBE_INTERFACE}.nmconnection
[connection]
id=${KUBE_INTERFACE}
uuid=${uuid}
type=ethernet
autoconnect-priority=-999
interface-name=${KUBE_INTERFACE}
timestamp=${timestamp}

[ethernet]
mac-address=""

[ipv4]
address1=${NETID}.${2},${GATEWAY}
dns=8.8.8.8;
method=manual

[ipv6]
addr-gen-mode=eui64
method=disabled

[proxy]
EOF" && echo -e "NetworkManager setting\n" && sudo cat /etc/NetworkManager/system-connections/${KUBE_INTERFACE}.nmconnection

#Ubuntu NetworkManager setting
[ "${OS_VER}" == "xUbuntu_22.04" ] && sudo bash -c "cat << EOF > /etc/netplan/00-installer-config.yaml
network:
  version: 2
  ethernets:
    ${KUBE_INTERFACE}:
      dhcp4: no
      dhcp6: no
      addresses: [${NETID}.${2}]
      routes:
      - to: default
        via: ${GATEWAY}
      nameservers:
        addresses: [8.8.8.8]
EOF" && echo -e "netplane setting\n" && cat /etc/netplan/00-installer-config.yaml; echo
  fi
;;

set-hostname) #Setup hostname. [ hostname [ name ] ]
  [ -z ${2} ] && echo -e "${RED}Please input parameter after ${YELLOW}\"set-hostname\"${NC} [ <hostname> ]\n" && exit || echo "$2" | sudo tee /etc/hostname &> /dev/null

  echo -en " [${GREEN}●${NC}] Set hostname to: "
  cat /etc/hostname && echo
  interrupt
  echo "reboot node..." && sleep 1.5 && sudo reboot
;;

set-ver) #Set kube*、cri-o package version.
  set-environment-variable
  setup-pkg-version ${@}
  set-environment-variable
  crio-available-version-check
;;

set-sc) #Set the preferred storageClass for deploy.
  [ -z ${2} ] && echo -e "Input parameter ${YELLOW}[ rook-ceph-block | local-path ]${NC}" && exit
  set-environment-variable
  sed -i "s/STORAGE_CLASS=\"${STORAGE_CLASS}\"/STORAGE_CLASS=\"${2}\"/g" ~/bin/kdm_function
  source ~/bin/kdm_function
  set-environment-variable
  echo -e "storageClass: ${YELLOW}${STORAGE_CLASS}${NC}"
;;

set-context) #Set the context's cluster and user name.
  [ "${#}" != "3" ] && echo -e "Input parameter ${YELLOW}[ Cluster-name | User-name ]${NC}" && exit
  set-environment-variable

  sed -i "s/CLUSTER_NAME=${CLUSTER_NAME}/CLUSTER_NAME=${2}/g" ~/bin/kdm_function
  ls ~/.kube/config &> /dev/null
  [ ${?} == 0 ] && sed -i "s/${CLUSTER_NAME}/${2}/g" ~/.kube/config

  sed -i "s/CLUSTER_USER=${CLUSTER_USER}/CLUSTER_USER=${3}/g" ~/bin/kdm_function
  ls ~/.kube/config &> /dev/null
  [ ${?} == 0 ] && sed -i "s/${CLUSTER_USER}/${3}/g" ~/.kube/config

  source ~/bin/kdm_function
  set-environment-variable
  echo -e "Cluster: ${YELLOW}${CLUSTER_NAME}${NC}"
  echo -e "User: ${YELLOW}${CLUSTER_USER}${NC}"
;;

set-coredns) #Change core-dns domain setting.
  [ "${#}" != "2" ] && echo -e "Input parameter ${YELLOW}[ dns-domain-name ]${NC}" && exit
  set-environment-variable
  sed -i "s/CORE_DNS_DOMAIN=${CORE_DNS_DOMAIN}/CORE_DNS_DOMAIN=${2}/g" ~/bin/kdm_function
  source ~/bin/kdm_function
  set-environment-variable
  echo -e "CORE_DNS_DOMAIN: ${YELLOW}${CORE_DNS_DOMAIN}${NC}"
;;

set-selinux) #Setting SELinux mod [ set | apply ]
[ -z ${2} ] && echo -e "${RED}Please input parameter after ${YELLOW}\"set-selinux\" [ set | apply ]${NC}" && exit
  if [ "${2}" == "set" ]
    then
      [ -z ${3} ] && echo -e "${RED}Please input parameter after ${YELLOW}\"set\" [ disable | permissive | enforcing ]${NC}" && exit
      echo "${3}" | grep -vE 'disable|permissive|enforcing' &> /dev/null
      [ ${?} == 0 ] && echo -e "${RED}Please input parameter after ${YELLOW}\"set\" [ disable | permissive | enforcing ]${NC}" && exit

      [ "${OS_VER}" == "RHEL_8" ] && sudo sed -i "/^export\ SELINUX_MODE=/c\export\ SELINUX_MODE=${3}" ~/bin/kdm
      [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo sed -i "/^SELINUX_MODE=/c\SELINUX_MODE=${3}" ~/bin/kdm
    
      [ "${OS_VER}" == "RHEL_8" ] && echo -e "${YELLOW}${install_list}${NC}" && cat ~/bin/kdm | grep '^export\ SELINUX_MODE='
      [ "${OS_VER}" == "CentOS_8_Stream" ] && echo -e "${YELLOW}${install_list}${NC}" && cat ~/bin/kdm | grep '^export\ SELINUX_MODE='
  elif [ "${2}" == "apply" ]
    then
      node-selector hosts
      for install_list in ${wk_nodes} ${cp_nodes}
        do
          echo "${OS_VER}" | grep -E "RHEL_8|CentOS_8_Stream" &> /dev/null
          [ ${?} == 0 ] && status=1 || status=0
          echo -e "${install_list}"

          cat ~/bin/kdm | grep '^export\ SELINUX_MODE=' | grep -E 'permissive|enforcing' &> /dev/null
          [ ${?} == 0 ] && [ "${status}" == "1" ] && ssh ${install_list} sudo setenforce ${SELINUX_MODE}
          
          [ "${status}" == "1" ] && ssh ${install_list} "sudo sed -i "/^SELINUX=/c\SELINUX=${SELINUX_MODE}" /etc/selinux/config"
          [ "${status}" == "1" ] && ssh ${install_list} cat /etc/selinux/config | grep '^SELINUX='
        done
  fi
;;

sync-ssh) #scp .ssh to every nodes.
  rm ~/.ssh/known_hosts
  node-selector hosts
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}Send to ${install_list}${NC}"
      scp -r .ssh ${install_list}: &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] Sync successed" || echo -e "[${RED}○${NC}] Sync unsuccess"
    done && echo
;;

sync-kdm) #scp kdm to every nodes.
  node-selector hosts
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      ssh ${install_list} "ls ~/bin &> /dev/null"
      [ ${?} != 0 ] && ssh ${install_list} "mkdir ~/bin &> /dev/null"
      echo -e "${YELLOW}Send to ${install_list}${NC}"
      scp bin/kdm ${install_list}:bin/ &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] kdm sync successed" || echo -e "[${RED}○${NC}] kdm sync unsuccess"
      scp bin/kdm_function ${install_list}:bin/ &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] kdm_function sync successed" || echo -e "[${RED}○${NC}] kdm_function sync unsuccess"
    done && echo
;;

sync-yaml) #scp yaml to every nodes.
  node-selector hosts
  for install_list in ${cp_nodes}
    do
      echo -e "${YELLOW}Send to ${install_list}${NC}"
      scp -r yaml ${install_list}: &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] Sync successed" || echo -e "[${RED}○${NC}] Sync unsuccess"
    done && echo
;;

sync-kube-config) #scp yaml to every nodes.
  node-selector hosts
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}Send to ${install_list}${NC}"
      scp -r .kube ${install_list}: &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] Sync successed" || echo -e "[${RED}○${NC}] Sync unsuccess"
    done && echo
;;

pkg-ver) #Check package repositories.
  set-environment-variable
  [ -z ${2} ] && node-message ${@} && exit
  node-selector ${@}
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}${install_list} | package repositories check${NC}"
      [ "${OS_VER}" == "RHEL_8" ] && echo -e "${YELLOW}Package enabled repositories:${NC}"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "sudo dnf repolist --enabled -q"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && echo -e "${YELLOW}Package enabled repositories:${NC}"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "sudo dnf repolist --enabled -q"
      for plist in ${PACKAGE_LIST}
        do
          echo -e "${YELLOW} ${plist} pkg-version:${NC}"
          [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "echo ${plist} | grep -v 'kube' &> /dev/null && sudo dnf provides ${plist} 2> /dev/null | grep 'Provide' | head -n 5 | cut -d '=' -f 2 | sed 's/^.//'"
          [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "echo ${plist} | grep 'kube' &> /dev/null && sudo dnf provides ${plist} --disableexcludes=kubernetes 2> /dev/null | grep 'Provide' | grep ${CRIO_RELEASE} | cut -d '=' -f 2 | sed ':a;N;s/\n/ |/g;ta' | sed 's/^.//'"
          [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "echo ${plist} | grep -v 'kube' &> /dev/null && sudo dnf provides ${plist} 2> /dev/null | grep 'Provide' | head -n 5 | cut -d '=' -f 2 | sed 's/^.//'"
          [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "echo ${plist} | grep 'kube' &> /dev/null && sudo dnf provides ${plist} --disableexcludes=kubernetes 2> /dev/null | grep 'Provide' | grep ${CRIO_RELEASE} | cut -d '=' -f 2 | sed ':a;N;s/\n/ |/g;ta' | sed 's/^.//'"
          [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "sudo apt-cache madison ${plist} | head -n 5"
        done
  done
;;

pkg-repo) #Setup package repositories. [ add | rm ]
  [ -z ${2} ] && echo -e "${RED}Please input parameter. [ add | rm ]${NC}" && exit
  [ -z ${3} ] && node-message ${@} && exit

  node-selector ${@}

  interrupt ${RED}Modify packages repository on nodes: ${YELLOW}${cp_nodes} ${wk_nodes}${NC}

  if [ "${2}" == "add" ]
    then
      echo -e "${YELLOW}Prepare to add package repositories.${NC}"
      for install_list in ${cp_nodes} ${wk_nodes}
        do
          echo -e "${YELLOW}${install_list} | repositories procedure${NC}"
          package-repository-add && echo
        done
    elif [ "${2}" == "rm" ]
      then
        echo -e "${YELLOW}Prepare to remove package repositories.${NC}"
        for install_list in ${cp_nodes} ${wk_nodes}
          do
            [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "ls /etc/yum.repos.d/devel:kubic* > /dev/null 2>&1" && ssh ${install_list} "sudo rm /etc/yum.repos.d/devel:kubic*"
            [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "sudo rm /etc/yum.repos.d/kubernetes* > /dev/null 2>&1"
            [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "sudo dnf -y update --refresh" &> /dev/null
            [ "${OS_VER}" == "RHEL_8" ] && echo -e " [${GREEN}●${NC}] ${install_list} package repositories has been removed"
            [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "ls /etc/yum.repos.d/devel:kubic* > /dev/null 2>&1" && ssh ${install_list} "sudo rm /etc/yum.repos.d/devel:kubic*"
            [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "sudo rm /etc/yum.repos.d/kubernetes* > /dev/null 2>&1"
            [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "sudo dnf -y update --refresh" &> /dev/null
            [ "${OS_VER}" == "CentOS_8_Stream" ] && echo -e "${GREEN}${install_list} package repositories has been removed${NC}"
            [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "ls /etc/apt/sources.list.d/* > /dev/null 2>&1" && ssh ${install_list} "sudo rm /etc/apt/sources.list.d/*"
            [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "ls /etc/apt/trusted.gpg.d/devel_kubic_libcontainers_unstable.gpg > /dev/null 2>&1" && ssh ${install_list} "sudo rm /etc/apt/trusted.gpg.d/devel_kubic_libcontainers_unstable.gpg"
            [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "ls /etc/apt/sources.list.d/devel:kubic:libcontainers:unstable.list > /dev/null 2>&1" && ssh ${install_list} "sudo rm /etc/apt/sources.list.d/devel:kubic:libcontainers:unstable.list"
            [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "sudo apt -y update" &> /dev/null
            [ "${OS_VER}" == "xUbuntu_22.04" ] && echo -e " [${GREEN}●${NC}] ${install_list} package repositories has been removed"
          done
  fi
;;

pkg-install) #Install basic package & setup environment.
  [ -z ${2} ] && node-message ${@} && exit

  check_list=${@}
  node-selector ${@}
  interrupt ${RED}Install packages on nodes: ${YELLOW}${cp_nodes} ${wk_nodes}${NC} && clear
  set-environment-variable
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      ssh ${install_list} 'hostname' &> /dev/null 2>&1
      [ ${?} != 0 ] && echo -e " [${RED}●${NC}] This node not available\n" && continue
      echo -e "${YELLOW}${install_list} | package install procedure${NC}"

      system-swapoff
      system-firewalld
      system-selinux
      system-modules
      system-enable-ipv4-forward
      system-disable-ipv6

      #Add crio、kubernetes package repositories
      package-repository-add
      host-os-detection
      package-install-crio
      package-configure-crio
      image-repository-setting

      #Kubernetes package setup
      #[ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "sudo apt install -qy apt-transport-https --yes" &> /dev/null

      cri_current_ver=$(crio version 2>&1 | grep "^Version" | awk '{ print $2 }')
      [ "${OS_VER}" == "CentOS_8_Stream" ] && sudo dnf provides kubectl --disableexcludes=kubernetes | grep 'Provide' | grep ${CRIO_RELEASE} | cut -d '=' -f 2 | grep ${cri_current_ver} &> /dev/null
      [ ${?} == 0 ] && KUBE_SUBVER=${cri_current_ver}

      echo ${install_list} | grep 'm' &> /dev/null
      [ ${?} == 0 ] && install="1" || install="0"
      if [ "${install}" == "1" ]
        then
          package-install-kube-package
          package-install-helm
          package-install-podman
          package-install-skopeo
          package-install-k9s
          #enable crio、kubelet
          daemon-enable ${install_list}
      fi

      echo ${install_list} | grep 'w' &> /dev/null
      [ ${?} == 0 ] && install="1" || install="0"
      if [ "${install}" == "1" ]
        then
          package-install-kube-package
          package-install-podman
          #enable crio、kubelet
          daemon-enable ${install_list}
      fi
    done
  echo -e "${YELLOW}Package Check list${NS}"
  package-check ${check_list}
;;

pkg-rm) #Remove basic package & setup environment.
  #Set variable
  set-environment-variable
  [ -z ${2} ] && node-message && exit

  node-selector ${@}

  interrupt ${RED}Please confirm this command will destruction node:${YELLOW} ${cp_nodes} ${wk_nodes}!${NC}

  echo -n "Processing"; echo -n "."; sleep 0.5; echo -n "."; sleep 0.5; echo "."; sleep 0.5
  for remove_list in ${cp_nodes}
    do
      echo -e "\n${YELLOW}${remove_list} | package removing${NC}"
      package-remove-skopeo
      package-remove-podman
      package-remove-helm
      package-remove-k9s
      package-remove-kube
      package-remove-crio
      package-remove-daemon-reload
    done

  for remove_list in ${wk_nodes}
    do
      echo -e "\n${YELLOW}${remove_list} | package removing${NC}"
      package-remove-podman
      package-remove-kube
      package-remove-crio
      package-remove-daemon-reload
    done
  node-power reboot ${@}
;;

pkg-check) #Check node package status.
  package-check ${@}
;;

pkg-fix) #Fix Ubuntu pkg.
  #Set variable
  set-environment-variable
  node-selector ${@}
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo mv /var/lib/dpkg/info/ /var/lib/dpkg/info_old/
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo mkdir /var/lib/dpkg/info/
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo apt update

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo apt -f install

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo mv /var/lib/dpkg/info/* /var/lib/dpkg/info_old/
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo rm -rf /var/lib/dpkg/info
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo mv /var/lib/dpkg/info_old/ /var/lib/dpkg/info/
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo apt update
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} sudo apt upgrade
    done
;;

docker-install) #Install docker.
  package-install-docker ${2}
;;

docker-compose-install) #Install docker-compose.
  package-install-docker ${2}
;;

k9s-install) #Install k9s on every control-plane.
  kubernetes-node-variable
  for clist in ${CP_NODES}
    do
      #install k9s
      ssh ${clist} "ls -al ~/bin/k9s" &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] ${clist} k9s already installed." && continue || ssh ${clist} "curl -sS https://webinstall.dev/k9s | timeout 20 bash" &> /dev/null
      #k9s path setup
      ssh ${clist} "mv ~/.local/bin/k9s ~/bin/" &> /dev/null
      ssh ${clist} "cat /etc/environment | grep "/home/${USER}/bin" &> /dev/null" &> /dev/null
      [ ${?} != 0 ] && ssh ${clist} "cat /etc/environment | tr -s ':' '\n' | sed 's/\/snap\/bin\"/\/snap\/bin\n\/home\/${USER}\/bin\"/g' | tr -s '\n' ':' | sed '$ s/.$//' | sudo tee /etc/environment &> /dev/null" &> /dev/null
      ssh ${clist} "cat /etc/profile | grep 'export KUBE_EDITOR=nano' &> /dev/null" &> /dev/null
      [ ${?} != 0 ] && ssh ${clist} "echo "export KUBE_EDITOR=nano" | sudo tee -a /etc/profile" &> /dev/null
      ssh ${clist} "cat /etc/profile | grep 'export K9S_EDITOR=nano' &> /dev/null" &> /dev/null
      [ ${?} != 0 ] && ssh ${clist} "echo "export K9S_EDITOR=nano" | sudo tee -a /etc/profile" &> /dev/null
      ssh ${clist} "rm -r ~/Downloads/" &> /dev/null
      ssh ${clist} "ls -al ~/bin/k9s" &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] ${clist} k9s installed"
    done
;;

k9s-rm) #Remove k9s on every control-plane.
  kubernetes-node-variable
  for clist in ${CP_NODES}
    do
      #delete k9s
      ssh ${clist} "rm ~/bin/k9s" &> /dev/null
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] ${clist} k9s removed" || echo -e " [${RED}●${NC}] ${clist} k9s not detected"
    done
;;

daemon-enable) #
  [ -z ${2} ] && node-message ${@} && exit

  check_list=${@}
  node-selector ${@}
  interrupt ${RED}Install packages on nodes: ${YELLOW}${cp_nodes} ${wk_nodes}${NC} && clear

  for install_list in ${wk_nodes} ${cp_nodes}
    do
      echo -e "${YELLOW}${install_list} | daemon status${NC}"
      daemon-enable ${install_list}
      echo
    done
;;

daemon-reload) #Reload system daemon.
  node-selector ${@}
  for restart_list in ${cp_nodes} ${wk_nodes}
    do
      daemon-reload ${restart_list}
    done
;;

cp-init) #Init first control-plane node & deploy CNI. [ calico or flannel ]
  #Set variable
  set-environment-variable
  
  #Check papameter [ high | single ]
  #POD_CIDR 172.16.0.0/17 [ 172.16.0.1 - 172.16.127.254 ]
  #SVC_CIDR 172.16.128.0/17 [ 172.16.128.1 - 172.16.255.254 ]
  #kdm cp-init high calico cidr
  [ ${#} != 3 ] && echo -e "${RED}Please input parameter after ${YELLOW}\"cp-init\" \n > [ high or single | calico or flannel | optional: pod 0.0.0.0/0 | optional: svc 0.0.0.0/0 ]${NC}" && exit
  test=$(echo ${@} | tr -s ' ' '\n' | grep -vE 'cp-init|high|single|calico|flannel|pod|svc' | tr -s '\n' ' ' | sed 's/.$//')

  echo "${2}" | grep -E 'high|single' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}\"${2}\"${RED} is not effective parameter! Please input parameter after ${YELLOW}\"cp-init\"\n > [ high or single ]${NC}" && exit
  echo "${3}" | grep -E 'calico|flannel' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}\"${3}\"${RED} is not effective parameter! Please input parameter after ${YELLOW}\"cp-init\"\n > [ calico or flannel ]${NC}" && exit

  interrupt ${RED}Please confirm this command will initialize kubernetes via ${YELLOW}`hostname`.${NC}

  #KUBE_VIP setup
  [ ${2} == "high" ] && ls /etc/kubernetes/manifests &> /dev/null 
  [ ${?} != 0 ] && [ ${2} == "high" ] && sudo mkdir -p /etc/kubernetes/manifests
  [ ${2} == "high" ] && curl https://raw.githubusercontent.com/kube-vip/kube-vip/main/docs/manifests/v0.4.1/kube-vip-arp.yaml &> /dev/null
  [ ${2} == "high" ] && [ ${?} == 0 ] && wget -qO - https://raw.githubusercontent.com/kube-vip/kube-vip/main/docs/manifests/v0.4.1/kube-vip-arp.yaml | sed "s/:v[0-9].[0-9].[0-9]/:${KUBE_VIP_VER}/g" | sed "s|eth0|${KUBE_INTERFACE}|g" | sed "s|192.168.0.1|${KUBE_VIP}|g" | sed "s|imagePullPolicy\: Always|imagePullPolicy\: IfNotPresent|g" | sudo tee /etc/kubernetes/manifests/kube-vip-arp.yaml &> /dev/null
  [ ${2} == "high" ] && ls ~/yaml/system &> /dev/null
  [ ${?} != 0 ] && [ ${2} == "high" ] && mkdir -p ~/yaml/system
  [ ${2} == "high" ] && sudo cp /etc/kubernetes/manifests/kube-vip-arp.yaml ~/yaml/system/
  [ ${2} == "high" ] && sudo chown ${USER}:${USER} ~/yaml/system/kube-vip-arp.yaml
  
  KUBEADM_VER=$(kubeadm version -o yaml | grep "gitVersion:" | head -n 1 | awk '{ print $2 }' | sed 's|v||g')
  [ "${KUBE_INIT_VER}" != "${KUBEADM_VER}" ] && KUBE_INIT_VER=$(kubeadm version -o yaml | grep "gitVersion:" | head -n 1 | awk '{ print $2 }' | sed 's|v||g')

  #Kubernetes setup
  #calico service & pod network
  [ "${3}" == "calico" ] && POD_CIDR=172.16.0.0/17
  [ "${3}" == "calico" ] && SVC_CIDR=172.16.128.0/17
  #[ "${3}" == "calico" ] && POD_CIDR=10.85.0.0/16
  #[ "${3}" == "calico" ] && SVC_CIDR=10.96.0.0/12
  [ ${2} == "high" ] && [ "${3}" == "calico" ] && sudo kubeadm init --image-repository=${KUBE_REGISTRY} --control-plane-endpoint=${KUBE_VIP}:6443 --pod-network-cidr=${POD_CIDR} --service-cidr=${SVC_CIDR} --service-dns-domain=${CORE_DNS_DOMAIN} --cri-socket=${CNI_SOCK} --upload-certs --kubernetes-version=${KUBE_INIT_VER} ${LOG_OUTPUT_LEVEL}
  [ ${2} == "single" ] && [ "${3}" == "calico" ] && sudo kubeadm init --pod-network-cidr=${POD_CIDR} --service-cidr=${SVC_CIDR} --service-dns-domain=${CORE_DNS_DOMAIN} --cri-socket=${CNI_SOCK} --upload-certs --kubernetes-version=${KUBE_INIT_VER} ${LOG_OUTPUT_LEVEL}
  [ ${?} != 0 ] && [ "${2}" == "calico" ] && interrupt ${RED}\nkubeadm init failure!${NC}
  [ "${3}" == "calico" ] && mkdir -p ${HOME}/.kube && sudo cp -i /etc/kubernetes/admin.conf ${HOME}/.kube/config && sudo chown $(id -u):$(id -g) ${HOME}/.kube/config
  [ "${3}" == "calico" ] && ls ${HOME}/.kube/config &> /dev/null
  #helm calico network
  [ "${3}" == "calico" ] && helm repo add projectcalico https://projectcalico.docs.tigera.io/charts
  [ "${3}" == "calico" ] && kubectl create namespace tigera-operator
  [ "${3}" == "calico" ] && helm install calico projectcalico/tigera-operator --version ${CALICO_VER} --namespace tigera-operator
  #[ ${?} != 0 ] && echo -e "kubeadm init failure.\n" && exit

  #flannel service & pod network
  [ "${3}" == "flannel" ] && POD_CIDR=172.16.0.0/17
  [ "${3}" == "flannel" ] && SVC_CIDR=172.16.128.0/17
  #[ "${3}" == "flannel" ] && POD_CIDR=10.244.0.0/16
  #[ "${3}" == "flannel" ] && SVC_CIDR=10.98.0.0/24
  [ "${3}" == "flannel" ] && sudo kubeadm init --image-repository=${KUBE_REGISTRY} --control-plane-endpoint=${KUBE_VIP}:6443 --pod-network-cidr=${POD_CIDR} --service-cidr=${SVC_CIDR} --service-dns-domain=${CORE_DNS_DOMAIN} --cri-socket=${CNI_SOCK} --upload-certs --kubernetes-version=${KUBE_INIT_VER} ${LOG_OUTPUT_LEVEL}
  [ ${2} == "single" ] && [ "${3}" == "flannel" ] && sudo kubeadm init --image-repository=${KUBE_REGISTRY} --pod-network-cidr=${POD_CIDR} --service-cidr=${SVC_CIDR} --service-dns-domain=${CORE_DNS_DOMAIN} --cri-socket=${CNI_SOCK} --upload-certs --kubernetes-version=${KUBE_INIT_VER} ${LOG_OUTPUT_LEVEL}
  [ ${?} != 0 ] && [ "${2}" == "flannel" ] && interrupt ${RED}\nkubeadm init failure!${NC}
  [ "${3}" == "flannel" ] && mkdir -p ${HOME}/.kube && sudo cp -i /etc/kubernetes/admin.conf ${HOME}/.kube/config && sudo chown $(id -u):$(id -g) ${HOME}/.kube/config
  [ "${3}" == "flannel" ] && ls ${HOME}/.kube/config &> /dev/null
  [ "${3}" == "flannel" ] && kubectl apply -f https://raw.githubusercontent.com/Bookman-W/kdm/master/yaml/kube-flannel.yaml
  #[ ${?} != 0 ] && echo -e "kubeadm init failure.\n" && exit

  #Change context name & username
  sed -i "s/kubernetes-admin/${CLUSTER_USER}/g" ~/.kube/config
  sed -i "s/kubernetes/${CLUSTER_NAME}/g" ~/.kube/config

  #taint setup
  #KUBE_CURRENT_VER=$(kubectl get nodes | grep "control-plane" | awk '{ print $5 }' | head -n 1)
  #kubectl taint node `hostname` node-role.kubernetes.io/control-plane:NoSchedule-
  #echo "${KUBE_CURRENT_VER}" | grep -E '1.25|1.26' &> /dev/null
  #[ ${?} != 0 ] && kubectl taint node `hostname` node-role.kubernetes.io/master:NoSchedule-
  echo && k9s -c pods -A
;;

cp-join) #Let control-plane nodes join cluster. [ hosts | node ]
  #Set variable
  set-environment-variable
  [ -z ${2} ] && cp-node-message ${@} && exit

  node-selector ${@}

  if [ "${2}" == "hosts" ]
    then
      cluster_list=$(kubectl get nodes | tail -n +2 | awk '{ print $1 }' | tr -s '\n' '|' | sed '$ s/.$//')
      cp_nodes=$(echo -e "`cat /etc/hosts | grep -vE '#|ip6|${CORE_DNS_DOMAIN}' | grep "${NETID}" | awk '{ print $2 }' | grep -vE ${cluster_list}| tr -s ' ' '\n' | grep "\-m" | tr -s '\n' ' '`\n")
      echo ${cp_nodes} | grep -n '^$' &> /dev/null && cp_nodes="none "
    else
      shift
      list="${@}"
      cp_nodes=$(echo "${list}" | tr -s ' ' '\n' | grep "\-m" | tr -s '\n' ' ')
  fi
  interrupt ${RED}Please confirm this command will let ${YELLOW}${cp_nodes}${RED}join cluster!${NC}
  [ "${cp_nodes}" == "none " ] && echo -e "${RED}Command canceled${NC}" && exit

  ls ~/yaml &> /dev/null
  [ ${?} != 0 ] && mkdir -p ~/yaml/system && cp /etc/kubernetes/manifests/kube-vip-arp.yaml ~/yaml/system

  certs=$(sudo kubeadm init phase upload-certs --upload-certs | tail -n 1)
  JOIN=$(echo "sudo `kubeadm token create --print-join-command 2>/dev/null`")
  for clist in ${cp_nodes}
    do
      echo -e "${YELLOW}${clist} | join procedure${NC}"
      ssh ${clist} "ls ~/yaml/system &> /dev/null"
      [ ${?} != 0 ] && ssh ${clist} "mkdir -p ~/yaml/system"
      scp ~/yaml/system/kube-vip-arp.yaml ${clist}:yaml/system/
      ssh ${clist} "${JOIN} --control-plane --certificate-key ${certs} ${LOG_OUTPUT_LEVEL}"
      [ ${?} != 0 ] && interrupt ${RED}\nkubeadm join failure!${NC}
      echo -en "${GREEN}"

      #taint setup
      #kubectl taint node ${clist} node-role.kubernetes.io/control-plane:NoSchedule-
      #KUBE_CURRENT_VER=$(kubectl get nodes | grep "control-plane" | awk '{ print $5 }' | head -n 1)
      #echo "${KUBE_CURRENT_VER}" | grep -E '1.25|1.26' &> /dev/null
      #[ ${?} != 0 ] && kubectl taint node `hostname` node-role.kubernetes.io/master:NoSchedule-

      echo -en "${NC}"
      ssh ${clist} "ls /etc/kubernetes/manifests" &> /dev/null
      [ ${?} != 0 ] && ssh ${clist} "sudo mkdir -p /etc/kubernetes/manifests"
      ssh ${clist} "sudo cp ~/yaml/system/kube-vip-arp.yaml /etc/kubernetes/manifests/kube-vip-arp.yaml"
      ssh ${clist} "mkdir -p ${HOME}/.kube"
      scp .kube/config ${clist}:.kube/
    done && k9s -c pods -A
;;

wk-join) #Let worker nodes join cluster. [ hosts | node ]
  #Set variable
  set-environment-variable
  [ -z ${2} ] && wk-node-message ${@} && exit
  if [ "${2}" == "hosts" ]
    then
      cluster_list=$(kubectl get nodes | tail -n +2 | awk '{ print $1 }' | tr -s '\n' '|' | sed '$ s/.$//')
      wk_nodes=$(echo -e "`cat /etc/hosts | grep -vE '#|ip6|${CORE_DNS_DOMAIN}' | grep "${NETID}" | awk '{ print $2 }' | grep -vE ${cluster_list}| tr -s ' ' '\n' | grep "\-w" | tr -s '\n' ' '`\n")
      echo ${wk_nodes} | grep -n '^$' &> /dev/null && wk_nodes="none "
    else
      shift
      list="${@}"
      wk_nodes=$(echo "${list}" | tr -s ' ' '\n' | grep "\-w" | tr -s '\n' ' ')
  fi

  interrupt ${RED}Please confirm this command will let ${YELLOW}${wk_nodes}${RED}join cluster!${NC}
  [ "${wk_nodes}" == "none " ] && echo -e "${RED}Command canceled${NC}" && exit

  JOIN=$(echo "sudo `kubeadm token create --print-join-command 2>/dev/null`")
  for wlist in ${wk_nodes}
    do
      echo -e "${YELLOW}${wlist} | join procedure${NC}"
      ssh ${wlist} "${JOIN} ${LOG_OUTPUT_LEVEL}"
      echo -en "${GREEN}"
      kubectl label node ${wlist} node-role.kubernetes.io/worker=
      echo -en "${NC}"
    done && k9s -c pods -A
;;

cni-deploy) #Deploy kubernetes CNI. [ calico or flannel ]
  #Set variable
  set-environment-variable
  interrupt ${RED}Please confirm there is no CNI running on kubernetes.${NC}
  if [ -z ${2} ]
    then
      echo -e "Please input cni project. [ calico or flannel ]\n"
    elif [ ${2} == "calico" ]
      then
        #helm calico network
        helm repo add projectcalico https://projectcalico.docs.tigera.io/charts
        kubectl create namespace tigera-operator
        helm install calico projectcalico/tigera-operator --version v3.24.1 --namespace tigera-operator
        k9s -n calico-system
    elif [ ${2} == "flannel" ]
      then
        #flannel network
        kubectl apply -f https://raw.githubusercontent.com/Bookman-W/kdm/master/yaml/kube-flannel.yaml
        k9s -n kube-system
  fi
;;

cni-rm) #Delete kubernetes CNI. [ calico or flannel ]
  #Set variable
  set-environment-variable
  #Set variable
  set-environment-variable
  if [ -z ${2} ]
    then
      echo -e "Please input cni project. [calico、flannel]\n"
    elif [ ${2} == "calico" ]
      then
        #helm calico network
        helm delete calico -n tigera-operator
        kubectl delete namespace tigera-operator
        helm repo remove projectcalico
    elif [ ${2} == "flannel" ]
      then
        #flannel network
        kubectl delete -f https://raw.githubusercontent.com/Bookman-W/kdm/master/yaml/kube-flannel.yaml
        for install_list in ${CP_NODES} ${WK_NODES}
          do
            ssh ${install_list} "sudo rm /etc/cni/net.d/*flannel*"
          done
  fi
;;

dns-rollout) #Rollout coredns & calico-api-server. [ if pod present ]
  #rollout coredns/calico-apiserver
  kubectl rollout restart deployment/coredns -n kube-system &> /dev/null
  while true
    do
      kubectl get pods -n kube-system | grep 'coredns' | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
      [ ${?} != 0 ] && break
    done
    echo -e " [${GREEN}●${NC}] coredns rollout"

  kubectl get ns | grep 'calico-apiserver' &> /dev/null
  [ ${?} == 0 ] && kubectl rollout restart deployment/calico-apiserver -n calico-apiserver &> /dev/null || exit

  while true
    do
      kubectl get pods -n calico-apiserver 2> /dev/null | grep 'calico-apiserver' | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
      [ ${?} != 0 ] && break
    done
    echo -e " [${GREEN}●${NC}] calico-apiserver rollout"
;;

csi-deploy) #Deploy kubernetes CSI. [ local-path | rook-ceph ]
  #Set variable
  set-environment-variable
  if [ -z ${2} ]
    then
      echo -e "Input parameter. ${YELLOW}[ local-path | rook-ceph ]${NC}"
    elif [ ${2} == "local-path" ]
      then
          #local-path-storage
          curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Local-path/local-path-storage.yaml | sed "s/<image-version>/${LOCAL_PATH_VER}/g" | kubectl apply -f -
          k9s -n local-path-storage
    elif [ ${2} == "rook-ceph" ]
      then
        which git &> /dev/null
        [ ${?} != 0 ] && echo "install git" && sudo dnf install -y git && clear
        #helm repo add rook-release https://charts.rook.io/release
        ls ~/yaml/rook/ &> /dev/null
        [ ${?} != 0 ] && mkdir ~/yaml/rook/
        ls rook/ &> /dev/null
        [ ${?} == 0 ] && sudo rm -r ~/rook && git clone --single-branch --branch ${ROOK_GIT_RELEASE} -b ${ROOK_TAG} https://github.com/rook/rook.git || git clone --single-branch --branch ${ROOK_GIT_RELEASE} -b ${ROOK_TAG} https://github.com/rook/rook.git
        #copy
        cp ~/rook/deploy/examples/crds.yaml ~/yaml/rook/
        cp ~/rook/deploy/examples/common.yaml ~/yaml/rook/
        cp ~/rook/deploy/examples/operator.yaml ~/yaml/rook/
        cp ~/rook/deploy/examples/cluster.yaml ~/yaml/rook/
        cp ~/rook/deploy/examples/pool.yaml ~/yaml/rook/
        cp ~/rook/deploy/examples/toolbox.yaml ~/yaml/rook/

        cp ~/rook/deploy/examples/csi/rbd/pvc.yaml ~/yaml/rook

        cp ~/rook/deploy/examples/csi/rbd/storageclass.yaml ~/yaml/rook/storageclass-block.yaml
        cp ~/rook/deploy/examples/csi/cephfs/pvc.yaml ~/yaml/rook/pvc-fs.yaml

        #if SELinux enabled, must be set this variable to true
        status=0
        [ "${OS_VER}" == "RHEL_8" ] && getenforce | grep -E 'Permissive|Enforcing' &> /dev/null
        [ "${OS_VER}" == "CentOS_8_Stream" ] && getenforce | grep -E 'Permissive|Enforcing' &> /dev/null
        [ ${?} == 0 ] && sed -i '/ROOK_HOSTPATH_REQUIRES_PRIVILEGED/{n;s/false/true/;}' ~/yaml/rook/operator.yaml
        
        [ "${OS_VER}" == "RHEL_8" ] && cat ~/yaml/rook/operator.yaml | grep -A 1 'ROOK_HOSTPATH_REQUIRES_PRIVILEGED' | grep 'value: "true"' &> /dev/null && status=1 || status=0
        [ "${OS_VER}" == "CentOS_8_Stream" ] && cat ~/yaml/rook/operator.yaml | grep -A 1 'ROOK_HOSTPATH_REQUIRES_PRIVILEGED' | grep 'value: "true"' &> /dev/null && status=1 || status=0
        [ "${OS_VER}" == "xUbuntu_22.04" ] && status=1
        [ "${status}" == "0" ] && echo -e " [${RED}●${NC}] ~/yaml/rook/operator.yaml setting is Incorrect. " && exit
        
        #apply yaml
        kubectl apply -f ~/yaml/rook/crds.yaml; sleep 2
        echo -e "${YELLOW}crds applied${NC}"
        kubectl apply -f ~/yaml/rook/common.yaml; sleep 2
        echo -e "${YELLOW}common applied${NC}"
        kubectl apply -f ~/yaml/rook/operator.yaml; sleep 2
        echo -e "${YELLOW}operator applied${NC}"
        kubectl apply -f ~/yaml/rook/cluster.yaml; sleep 2
        echo -e "${YELLOW}cluster applied${NC}"
        kubectl apply -f ~/yaml/rook/pool.yaml; sleep 2
        echo -e "${YELLOW}pool applied${NC}"
        kubectl apply -f ~/yaml/rook/toolbox.yaml; sleep 2
        echo -e "${YELLOW}toolbox applied${NC}"

        kubectl apply -f ~/yaml/rook/storageclass-block.yaml; sleep 3
        echo -e "${YELLOW}storageclass-block created${NC}"
        curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Rook-Ceph/ceph-deshboard.yaml | sed "s/<ingressClass>/${INGRESS_CLASS_NAME}/g" | sed "s/<URL>/${CEPH_DASHBOARD_URL}/g" | kubectl apply -f - 
        echo -e "${YELLOW}deshboard ingress applied${NC}"
        
        echo -e "\n${YELLOW}rook-ceph deployed, please wait OSDs deploy.${NC}"
        sleep 3
        k9s -c pods -n ${ROOK_CEPH_NS}
  fi
;;

csi-rm) #Delete kubernetes CSI. [ local-path | rook-ceph ]
  #Set variable
  set-environment-variable
  if [ -z ${2} ]
    then
      echo -e "Please input csi project. [ local-path | rook-ceph ]\n"
    elif [ ${2} == "local-path" ]
      then
        interrupt ${RED}Please confirm this command will destruction CSI ${YELLOW}${2} ${RED}!${NC}
        #local-path-storage
        curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Local-path/local-path-storage.yaml | sed "s/<image-version>/${LOCAL_PATH_VER}/g" | kubectl delete -f -
        [ ${?} == 0 ] && echo "local-path-storage deleted."
    elif [ ${2} == "rook-ceph" ]
      then
        pod_list=$(kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph | grep -v '^[0-9]' | grep -v '^[A-Z]' | awk '{ print $1 }')
        for list in ${pod_list}
          do
            kubectl -n ${ROOK_CEPH_NS} patch ${list} -p '{"metadata":{"finalizers": []}}' --type=merge
          done
        interrupt ${RED}Please confirm this command will destruction CSI ${YELLOW}${2} ${RED}!${NC}
        kubectl -n ${ROOK_CEPH_NS} patch cephcluster ${ROOK_CEPH_NS} --type merge -p '{"spec":{"cleanupPolicy":{"confirmation":"yes-really-destroy-data"}}}'
        kubectl delete CephBlockPool/replicapool -n ${ROOK_CEPH_NS}
        kubectl -n ${ROOK_CEPH_NS} delete cephcluster ${ROOK_CEPH_NS}

        curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Rook-Ceph/ceph-deshboard.yaml | sed "s/<URL>/${CEPH_DASHBOARD_URL}/g" | kubectl delete -f - 
        kubectl delete -f ~/yaml/rook/toolbox.yaml; sleep 2
        kubectl delete -f ~/yaml/rook/operator.yaml; sleep 2
        kubectl delete -f ~/yaml/rook/common.yaml; sleep 2
        kubectl delete -f ~/yaml/rook/crds.yaml; sleep 2
        kubectl delete namespace ${ROOK_CEPH_NS}
        kdm csi-rook wipe-data hosts
  fi
;;

csi-rook) #Check rook status or DataDir. [ status | dashboard-pw | data-check | lvm-status | wipe-data | disk-check ]
  if [ "$2" == "" ]
    then
      echo -e "${YELLOW}Please input parameter [ status | dashboard-pw | data-check | lvm-status | wipe-data | disk-check ]${NC}"
  elif [ "$2" == "status" ]
    then
      #rook version
      kubectl get ns | grep 'rook-ceph' &> /dev/null
      [ ${?} != 0 ] && echo -e " [${RED}●${NC}] rook-ceph not detected\n" && exit

      ROOK_GIT_VER=$(kubectl -n ${ROOK_CEPH_NS} get jobs -o jsonpath='{range .items[*]}{.metadata.name}{"  \tsucceeded: "}{.status.succeeded}{"      \trook-version="}{.metadata.labels.rook-version}{"\n"}{end}' | awk '{ print $4 }' | cut -d '=' -f 2 | grep "^v" | head -n 1)
      POD_NAME=$(kubectl -n ${ROOK_CEPH_NS} get pod -o custom-columns=name:.metadata.name --no-headers | grep rook-ceph-mon-b)
      CEPH_IMAGE=$(kubectl -n ${ROOK_CEPH_NS} get pod ${POD_NAME} -o jsonpath='{.spec.containers[0].image}')
      kubectl -n ${ROOK_CEPH_NS} get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}' > /dev/null 2>&1
      [ ${?} == 0 ] && CEPH_VER=$(echo -e "`kubectl -n ${ROOK_CEPH_NS} exec -it $(kubectl -n ${ROOK_CEPH_NS} get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- ceph --version | awk '{ print $3 }'`\n")
      
      echo -e "Rook-ceph git release: ${ROOK_GIT_VER}\nceph version: ${CEPH_VER} | image: ${CEPH_IMAGE}\n"
      
      #rook status
      [ ${?} == 0 ] && kubectl -n ${ROOK_CEPH_NS} exec -it $(kubectl -n ${ROOK_CEPH_NS} get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- ceph -s || echo -e "- rook-ceph not detected\n"
      kubectl -n ${ROOK_CEPH_NS} get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}' > /dev/null 2>&1
      [ ${?} == 0 ] && kubectl -n ${ROOK_CEPH_NS} exec -it $(kubectl -n ${ROOK_CEPH_NS} get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- ceph osd status || echo -e "- rook-ceph not detected\n"
  elif [ "$2" == "fix-mon" ]
    then
      kubectl get ConfigMap rook-config-override -n ${ROOK_CEPH_NS} -o yaml | sed 's/""/| /g' | sed 's/config: | /config: |\n    [global]\n    mon clock drift allowed = 0.5/g' > ~/yaml/rook/rook-config-override.yaml
      kubectl replace -f ~/yaml/rook/rook-config-override.yaml --force
      kubectl delete pods -n ${ROOK_CEPH_NS} $(kubectl get pods -n ${ROOK_CEPH_NS} -o custom-columns=NAME:.metadata.name --no-headers | grep 'mon')
  elif [ "$2" == "dashboard-pw" ]
    then
      kubectl -n ${ROOK_CEPH_NS} get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}' > /dev/null 2>&1
      [ ${?} == 0 ] && echo -e "`kubectl -n ${ROOK_CEPH_NS} get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode`\n" || echo -e "- rook-ceph not detected\n"
  elif [ "$2" == "data-check" ]
    then
      for install_list in ${CP_NODES} ${WK_NODES};
        do
          echo "${install_list} | Rook DataDir checking"
          ssh ${install_list} ls /var/lib/rook &> /dev/null
          [ ${?} == 0 ] && echo -e "Directory exist\n" || echo -e "- Directory not exist\n"
        done
  elif [ "$2" == "lvm-status" ]
    then
      for wlist in ${CP_NODES} ${WK_NODES};
        do
          echo "${wlist} | Node lvm-status"
          ssh ${wlist} sudo pvscan
          #[ ${?} == 0 ] && echo "rook-ceph signature has wiped" || echo "- rook-ceph signature not found"
          echo
        done
  elif [ "$2" == "wipe-data" ]
    then
      rook-wipe-data ${@}
  elif [ "$2" == "disk-check" ]
    then
      node-selector ${@}
      echo -e "${YELLOW}Check worker disk resource for rook-ceph${NC}"

      for wlist in ${wk_nodes}
        do
          disk_list=$(ssh ${wlist} sudo fdisk -l | grep '^Disk' | grep 'sd' | awk '{ print $2 }' | sed 's/://g;s/\/dev\///g' | tr -s '\n' ' ' | sed 's/.$//' | sed 's/sd/\/dev\/sd/g')
          for ceph_list in ${disk_list}
            do
              taget=""
              ssh ${wlist} sudo wipefs ${ceph_list} | grep 'ceph_bluestore' &> /dev/null
              [ ${?} == 0 ] && taget="/dev/"$(ssh ${wlist} sudo wipefs ${ceph_list} | grep 'ceph_bluestore' | awk '{ print $1 }') && break || continue
            done
          echo "${wlist}: ${taget}"
        done
  elif [ "$2" == "--" ]
    then
      shift && shift
      interrupt ${RED}Please confirm command ${YELLOW}${@}${RED} is correct.${NC}
      kubectl -n ${ROOK_CEPH_NS} exec -it $(kubectl -n ${ROOK_CEPH_NS} get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- ${@}
  fi
;;

controller-deploy) #Deploy basic service. [ metallb & nginx-ingress | Detect NETID just input xxx xxx ]
  #Set variable
  ls ~/yaml/controller &> /dev/null
  [ ${?} != 0 ] && mkdir -p ~/yaml/controller

  set-environment-variable
  if [ -z ${2} ]
    then
      echo -e "Please input parameter.\n"
      echo -e "controller: Setup basic service. [ metallb & ingress ] [ Automatic select networkID, package start end ]\n"

  elif [ -z ${3} ]
    then
      echo -e "Please input parameter.\n"
      echo -e "controller: Setup basic service. [ metallb & ingress ] [ Automatic select networkID, package start end ]\n"

    else
      for install_list in ${CP_NODES} ${WK_NODES}
        do
          echo -e "${YELLOW}${install_list} [ /etc/hosts ] setup${NC}"
          ssh ${install_list} sudo sed -i "/${NETID}.${2}/d" /etc/hosts &> /dev/null
          ssh ${install_list} cat /etc/hosts | grep "${NETID}.${2}"
          [ ${?} != 0 ] && ssh ${install_list} "echo "${NETID}.${2} ${CORE_DNS_DOMAIN}" | sudo tee -a /etc/hosts"
        done
      #metallb-system
      curl -s https://raw.githubusercontent.com/metallb/metallb/${METALLB_VER}/config/manifests/metallb-native.yaml | kubectl apply -f - 
      
      #Wait for all speaker deployed
      kubectl get po -n metallb-system | tail -n +2 | awk '{ print $2,$3 }' | grep -v '1/1 Running' &> /dev/null
      while [ ${?} == 0 ]
        do
          echo -e " [${YELLOW}●${NC}] Waiting for metallb deploy complete..."
          sleep 3
          kubectl get po -n metallb-system | tail -n +2 | awk '{ print $2,$3 }' | grep -v '1/1 Running' &> /dev/null || break
        done
      echo -e " [${GREEN}●${NC}] Metallb deploy completed."

      #Deploy ip-pool
      echo "Starting deploy metallb ip-pool"
      curl -s https://raw.githubusercontent.com/Bookman-W/kdm/master/yaml/metallb-pool.yaml | sed "s/<NETID>/${NETID}/g" | sed "s/<START>/${2}/g" | sed "s/<END>/${3}/g" | sed "s/<pool-name>/${METALLB_POOL_NAME}/g" | kubectl apply -f - 
      echo "metallb deployed!"

      #ingress-nginx
      #By helm
      #Create namespace
      helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
      helm repo update
      kubectl create ns ingress-nginx
      helm show values ingress-nginx/ingress-nginx --version ${INGRESS_NGINX_HELM_VER} > ~/yaml/controller/ingress-nginx.yaml
      #Config ingressClass name
      sed -i '0,/name: nginx/s/name: nginx/name: ingress-1/g' ~/yaml/controller/ingress-nginx.yaml
      #Deploy
      helm install ingress-nginx ingress-nginx/ingress-nginx -f ~/yaml/controller/ingress-nginx.yaml --namespace ingress-nginx

      #Old version
      #curl -s https://raw.githubusercontent.com/Bookman-W/kdm/master/yaml/ingress-nginx.yaml | sed "s/<ingress-name>/${INGRESS_CLASS_NAME}/g" | sed "s/<INGRESS_NGINX_VER>/${INGRESS_NGINX_VER}/g" | kubectl apply -f -
      #echo "ingress-nginx deployed!"

      k9s -c pods -A
  fi
;;

controller-rm) #Delete basic service. [ metallb & nginx-ingress ]
  #Set variable
  set-environment-variable
  #ingress-nginx
  curl -s https://raw.githubusercontent.com/Bookman-W/kdm/master/yaml/ingress-nginx.yaml | sed "s/<ingress-name>/${INGRESS_CLASS_NAME}/g" | kubectl delete -f -
  #metallb-system
  curl -s https://raw.githubusercontent.com/metallb/metallb/${METALLB_VER}/config/manifests/metallb-native.yaml | kubectl delete -f - 
  curl -s https://raw.githubusercontent.com/Bookman-W/kdm/master/yaml/metallb-pool.yaml | sed "s/<NETID>/${NETID}/g" | sed "s/<START>/${2}/g" | sed "s/<END>/${3}/g" | sed "s/<pool-name>/${METALLB_POOL_NAME}/g" | kubectl delete -f - 
;;

metrics-deploy) #Deploy metrics-server.
  #Set variable
  set-environment-variable
  #HA Version via helm
  helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
  helm install metrics-server metrics-server/metrics-server --set 'args={--kubelet-insecure-tls}' --version ${METRICS_VER} --namespace kube-system
  kubectl scale deploy/metrics-server --replicas=2 -n kube-system
  while true
    do
      kubectl get pods -n kube-system 2> /dev/null | grep '^metrics' | awk '{ print $2,$3 }' | grep '1/1 Running' &> /dev/null
      [ ${?} == 0 ] && break
      echo -e " [${YELLOW}●${NC}] Waiting for metrics deploy complete..."
      sleep 3
    done
  echo -e " [${GREEN}●${NC}] Metrics deploy completed."
;;

metrics-rm) #Delete metrics-server.
  #Set variable
  set-environment-variable
  helm delete metrics-server -n kube-system
  helm repo remove metrics-server
;;

prometheus-deploy) #Deploy prometheus.
  #Set variable
  set-environment-variable
  #追加並更新 helm repository
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm repo update

  #部署 operator 並驗證
  helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version ${PROMETHEUSE_VER} --namespace monitoring --create-namespace
  while true
    do
      kubectl get pods -n monitoring 2> /dev/null | awk '{ print $2,$3 }' | grep '1/1 Running' &> /dev/null
      [ ${?} == 0 ] && break
      echo -e " [${YELLOW}●${NC}] Waiting for kube-prometheus deploy complete..."
      sleep 3
    done
  echo -e " [${GREEN}●${NC}] Kube-prometheus deploy completed."
;;

prometheus-rm) #Delete prometheus.
  #Set variable
  set-environment-variable
  helm delete kube-prometheus-stack -n monitoring
  helm repo remove prometheus-community
  helm repo update:
;;

eck-deploy) #Deploy ECK
  #Set variable
  set-environment-variable
  node-selector hosts
  #Setup vm memory limit
  for setup_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}${setup_list} | ECK setup procedure${NC}"
      ssh ${setup_list} "cat /etc/sysctl.conf | grep 'vm.max_map_count=262144'" &> /dev/null
      [ ${?} != 0 ] && ssh ${setup_list} "echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf &> /dev/null" && echo "vm.max_map_count is setup." || echo "Already setup."
      ssh ${setup_list} "sudo sysctl --system | grep 'vm.max_map_count = 262144'" &> /dev/null
      [ ${?} == 0 ] && echo "vm.max_map_count is active." || echo "vm.max_map_count is deactivate."
      echo
    done
  #Setup password
  setup-password-control
  #Create crd & operator
  kubectl create -f https://download.elastic.co/downloads/eck/${ECK_OP_VER}/crds.yaml
  [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] crd created" || echo -e "[${YELLOW}○${NC}] crd not created"
  kubectl apply -f https://download.elastic.co/downloads/eck/${ECK_OP_VER}/operator.yaml
  [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] operator created" || echo -e "[${YELLOW}○${NC}] operator not created"
  #Create Superuser
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/filerealm-secret.yaml | sed 's/<user>/bigred/g' | sed "s/<password>/${ECK_PASSWORD}/g" | kubectl apply -f -
  [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] superuser secret created" || echo -e "[${YELLOW}○${NC}] superuser secret not created"
  #Create elasticsearch
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/elasticsearch.yaml | sed "s/<eck-version>/${ECK_VER}/g" | sed 's/<storageClassName>/rook-ceph-block/g' | kubectl apply -f -
  [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] Elasticsearch created" || echo -e "[${YELLOW}○${NC}] Elasticsearch not created"
  #Create Kibana & Ingress rule
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/kibana.yaml | sed "s/<eck-version>/${ECK_VER}/g" | kubectl apply -f -
  [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] Kibana created" || echo -e "[${YELLOW}○${NC}] Kibana not created"
  kubectl scale deployment/ek-test-kb -n elastic-system --replicas=2
  [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] Kibana scale to 2" || echo -e "[${YELLOW}○${NC}] Kibana not scale"
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/kibana-ingress.yaml | sed "s/<ingressClass>/${INGRESS_CLASS_NAME}/g" | kubectl apply -f -
  [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] Kibana-ingress created" || echo -e "[${YELLOW}○${NC}] Kibana-ingress not created"
  k9s -c pods -n elastic-system
;;

eck-rm) #Remove ECK
  #Set variable
  set-environment-variable
  node-selector hosts
  #Setup password
  setup-password-control
  #Delete Kibana & Ingress rule
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/kibana-ingress.yaml | kubectl delete -f -
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/kibana.yaml | sed "s/<eck-version>/${ECK_VER}/g" | kubectl delete -f -
  #Delete elasticsearch
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/elasticsearch.yaml | sed "s/<eck-version>/${ECK_VER}/g" | sed 's/<storageClassName>/rook-ceph-block/g' | kubectl delete -f -
  #Delete Superuser
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/filerealm-secret.yaml | sed 's/<user>/bigred/g' | sed "s/<password>/${ECK_PASSWORD}/g" | kubectl delete -f -
  #Delete crd & operator
  kubectl delete -f https://download.elastic.co/downloads/eck/${ECK_OP_VER}/operator.yaml
  kubectl delete -f https://download.elastic.co/downloads/eck/${ECK_OP_VER}/crds.yaml
    
  #Remove vm memory limit
  for setup_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}${setup_list} | ECK remove procedure${NC}"
      ssh ${setup_list} "sudo sed -i '/vm.max_map_count=262144/d' /etc/sysctl.conf" &> /dev/null && echo "Setting remove compelet."
      ssh ${setup_list} "sudo sysctl --system | grep 'vm.max_map_count = 262144'" &> /dev/null
      [ ${?} != 0 ] && echo "vm.max_map_count is deactivate." || echo "vm.max_map_count is activate."
      echo
    done
;;

eck-check) #Check ECK status
  while [ 1 ]
    do
      echo -e "${YELLOW}ElasticSearch status${NC}"
      kubectl get elasticsearch -n elastic-system 2> /dev/null
      [ ${?} != 0 ] && echo -e " [${RED}●${NC}] ElasticSearch not deploy" && echo
      echo -e "${YELLOW}Kibana status${NC}"
      kubectl get kibana -n elastic-system 2> /dev/null
      [ ${?} != 0 ] && echo -e " [${RED}●${NC}] Kibana not deploy" && echo
      sleep 2 && clear
    done
;;

mariadb-galera-deploy) #Deploy mariadb-galera
  #Set variable
  set-environment-variable
  #Setup password
  setup-password-control
  kubectl create namespace ${MARIADB_GALERA_NS}
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/MariaDB-Galera/mariadb-galera-secret.yaml -w '\n' | sed "s/<password>/${MARIADB_GALERA_PASSWORD}/g" | sed "s/<namespace>/${MARIADB_GALERA_NS}/g" | kubectl apply -f -
  #Add helm package
  helm repo list | grep 'bitnami' &> /dev/null
  [ ${?} != 0 ] && helm repo add bitnami https://charts.bitnami.com/bitnami

  #Get yaml file
  ls ~/yaml/mariadb-galera &> /dev/null
  [ ${?} != 0 ] && mkdir ~/yaml/mariadb-galera
  helm show values bitnami/mariadb-galera > ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #Fix yaml
  #storageClass: "" >> storageClass: "${STORAGE_CLASS}"
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "storageClass: \"${STORAGE_CLASS}\"" &> /dev/null
  [ ${?} != 0 ] && sed -i "s/storageClass: \"\"/storageClass: \"${STORAGE_CLASS}\"/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #Change clusterDomain
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "namespaceOverride: \"${MARIADB_GALERA_NS}\"" &> /dev/null
  [ ${?} != 0 ] && sed -i "s/clusterDomain: cluster.local/clusterDomain: ${CORE_DNS_DOMAIN}/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #namespaceOverride: "" >> namespaceOverride: "${MARIADB_GALERA_NS}"
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "namespaceOverride: \"${MARIADB_GALERA_NS}\"" &> /dev/null
  [ ${?} != 0 ] && sed -i "s/namespaceOverride: \"\"/namespaceOverride: \"${MARIADB_GALERA_NS}\"/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #type: ClusterIP >> type: LoadBalancer
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "type: LoadBalancer" &> /dev/null
  [ ${?} != 0 ] && sed -i "s/^  type: ClusterIP/  type: LoadBalancer/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #existingSecret: "" >> existingSecret: "mariadb-secret"
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "existingSecret: \"mariadb-secret\"" &> /dev/null
  [ ${?} != 0 ] && sed -i "s/existingSecret: \"\"/existingSecret: \"mariadb-secret\"/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #Remove Character set setting
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "collation_server=utf8_unicode_ci" &> /dev/null
  [ ${?} == 0 ] && sed -i "/collation_server=utf8_unicode_ci/d;/init_connect='SET NAMES utf8'/d;/character_set_server=utf8/d" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #Add Character set setting
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "collation_server=utf8_unicode_ci" &> /dev/null
  [ ${?} != 0 ] && sed -i "s/  ## Character set/  ## Character set\n  character-set-client-handshake=FALSE\n  collation_server=utf8mb4_unicode_ci\n  init_connect=\'SET NAMES utf8mb4 COLLATE utf8mb4_unicode_ci\'\n  character_set_server=utf8mb4/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #Add default-character-set=utf8mb4
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "default-character-set=utf8mb4" &> /dev/null
  [ ${?} != 0 ] && sed -i "360,/plugin_dir=/ s/plugin_dir=\/opt\/bitnami\/mariadb\/plugin$/plugin_dir=\/opt\/bitnami\/mariadb\/plugin\n  default-character-set=utf8mb4/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #user: "" >> user: "${MARIADB_GALERA_USER}"
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "user: \"${MARIADB_GALERA_USER}\"" &> /dev/null
  [ ${?} != 0 ] && sed -i "s/user: \"\"/user: \"${MARIADB_GALERA_USER}\"/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #wsrep_mode=REPLICATE_MYISAM >> wsrep_replicate_myisam=ON
  cat ~/yaml/mariadb-galera/mariadb-galera-values.yaml | grep "wsrep_mode=REPLICATE_MYISAM" &> /dev/null
  [ ${?} == 0 ] && sed -i "s/wsrep_mode=REPLICATE_MYISAM/wsrep_replicate_myisam=ON/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #annotations: {} >> annotations: metallb.universe.tf/loadBalancerIPs

  #replicaCount: [0-9] >> replicaCount: 3
  sed -i "s/replicaCount: [0-9]/replicaCount: 3/g" ~/yaml/mariadb-galera/mariadb-galera-values.yaml

  #Deploy mariadb-galera
  helm install mariadb-galera -f ~/yaml/mariadb-galera/mariadb-galera-values.yaml bitnami/mariadb-galera -n mariadb-galera

  k9s -c pods -n ${MARIADB_GALERA_NS}
  #Check command
  #cat yaml/mariadb-galera/mariadb-galera-values.yaml | grep -E -A 5 'namespaceOverride: |^  type: LoadBalancer|existingSecret: "|Character set|user: "|plugin_dir=|replicaCount: [0-9]|storageClass:'
;;

mariadb-galera-rm) #Remove mariadb-galera
  #Set variable
  set-environment-variable
  #Setup password
  setup-password-control
  kubectl scale sts mariadb-galera -n ${MARIADB_GALERA_NS} --replicas=0
  list=$(kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n ${MARIADB_GALERA_NS} | grep -v '^[0-9]' | grep -v '^[A-Z]' | awk '{ print $1 }')
    for i in ${list}
      do
        kubectl -n ${MARIADB_GALERA_NS} patch ${i} -p '{"metadata":{"finalizers": []}}' --type=merge
      done
  helm delete mariadb-galera -n mariadb-galera
  [ ${?} == 0 ] && kubectl delete ns ${MARIADB_GALERA_NS}
  helm repo remove bitnami
;;

harbor-deploy) #Deploy harbor

  #Add helm package
  helm repo add harbor https://helm.goharbor.io
  helm fetch harbor/harbor --untar

  #Get yaml file
  ls ~/yaml/harbor &> /dev/null
  [ ${?} != 0 ] && mkdir ~/yaml/harbor
  helm show values harbor/harbor > ~/yaml/harbor/harbor-values.yaml

;;

jenkins-deploy) #Deploy jenkins on kubernetes.
  #Set variable
  set-environment-variable
  kubectl create ns ${JENKINS_NS}
  kubectl create secret generic kubeconfig --from-file=/home/${USER}/.kube/config -n ${JENKINS_NS}
  #kubectl apply -f https://web.flymks.com/cicd/v1/jenkins.yaml
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Web-service/jenkins.yaml | sed 's/<image-name>/docker.io\/jenkins\/jenkins/g' | kubectl apply -f -
  k9s -c pods -n ${JENKINS_NS}
  kubectl get pods -n ${JENKINS_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}Jenkins deployed!${NC}" || echo -e "${RED}Jenkins not deploy!${NC}"
;;

jenkins-rm) #Delete jenkins on kubernetes.
  #Set variable
  set-environment-variable
  kubectl delete -f https://web.flymks.com/cicd/v1/jenkins.yaml
  kubectl delete ns ${JENKINS_NS}
  kubectl delete secret generic kubeconfig --from-file=/home/${USER}/.kube/config -n jenkins
;;

quay-deploy) #Deploy project-quay on kubernetes.
  #Set variable
  set-environment-variable
  #quay
  kubectl create ns ${QUAY_NS}
  #kubectl apply -f https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Web-service/quay.yaml
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Web-service/quay.yaml | sed "s/<storageclass>/${STORAGE_CLASS}/g" | kubectl apply -f -
  k9s -c pods -n ${QUAY_NS}
  kubectl get pods -n ${QUAY_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}Project Quay deployed!${NC}" || echo -e "${RED}Project Quay not deploy!${NC}"
;;

quay-rm) #Delete project-quay on kubernetes.
  #Set variable
  set-environment-variable
  kubectl delete -f https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/quay.yaml
  kubectl delete ns ${QUAY_NS}
;;

grafana-deploy) #Deploy grafana on kubernetes.
  #Set variable
  set-environment-variable
  #gf
  kubectl create ns ${GRAFANA_NS}
  #kubectl apply -f https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Web-service/grafana.yaml
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Web-service/grafana.yaml | sed "s/<storageclass>/${STORAGE_CLASS}/g" | kubectl apply -f -
  k9s -c pods -n ${GRAFANA_NS}
  kubectl get pods -n ${GRAFANA_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}Grafana deployed!${NC}" || echo -e "${RED}Grafana not deploy!${NC}"
;;

grafana-rm) #Delete grafana on kubernetes.
  #Set variable
  set-environment-variable
  #grafana
  kubectl delete -f https://web.flymks.com/grafana/v1/grafana.yaml
  kubectl delete ns ${GRAFANA_NS}
;;

landlord-deploy) #Deploy landlord on kubernetes.
  #Set variable
  set-environment-variable
  #ns & configmap
  kubectl create ns landlord
  kubectl create -n landlord configmap kuser-conf --from-file /home/${USER}/.kube/config

  #PVC
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/2-landlord-PVC.yaml | sed "s/<storageclass>/${STORAGE_CLASS}/g" | kubectl apply -f -
  echo "landlord PVC deployed!"

  #service
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/1-landlord-service.yaml | kubectl apply -f -
  k9s -c svc -n ${LANDLORD_NS}
  kubectl get svc -n ${LANDLORD_NS} | tail -n +2 | tr -s ' ' | cut -d ' ' -f 2 | grep -vE 'LoadBalancer|ClusterIP' &> /dev/null

  #gateway
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/3-landlord-gateway.yaml | kubectl apply -f -
  k9s -c pods -n ${LANDLORD_NS}
  kubectl get pods -n ${LANDLORD_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}gateway deployed!${NC}" || echo -e "${RED}gateway not deploy!${NC}"

  #kuser
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/4-landlord-kuser.yaml | kubectl apply -f -
  k9s -c pods -n ${LANDLORD_NS}
  kubectl get pods -n ${LANDLORD_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}kuser deployed!${NC}" || echo -e "${RED}kuser not deploy!${NC}"

  #logger
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/5-landlord-logger.yaml | kubectl apply -f -
  k9s -c pods -n ${LANDLORD_NS}
  kubectl get pods -n ${LANDLORD_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}logger deployed!${NC}" || echo -e "${RED}logger not deploy!${NC}"

  #mariadb
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/6-landlord-mariadb.yaml | kubectl apply -f -
  k9s -c pods -n ${LANDLORD_NS}
  kubectl get pods -n ${LANDLORD_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}mariadb deployed!${NC}" || echo -e "${RED}mariadb not deploy!${NC}"

  #tenant
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/7-landlord-tenant.yaml | sed "s/<storageclass>/${STORAGE_CLASS}/g" | kubectl apply -f -
  k9s -c pods -n ${LANDLORD_NS}
  kubectl get pods -n ${LANDLORD_NS} | tail -n +2 | awk '{ print $3 }' | grep -v 'Running' &> /dev/null
  [ ${?} != 0 ] && echo -e "${YELLOW}tenant deployed!${NC}" || echo -e "${RED}tenant not deploy!${NC}"
;;

landlord-rm) #Delete landlorsd on kubernetes.
  #Set variable
  set-environment-variable
  #tenant
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/7-landlord-tenant.yaml | sed "s/<storageclass>/${STORAGE_CLASS}/g" | kubectl delete -f -
  #mariadb
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/6-landlord-mariadb.yaml | kubectl delete -f -
  #logger
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/5-landlord-logger.yaml | kubectl delete -f -
  #kuser
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/4-landlord-kuser.yaml | kubectl delete -f -
  #gateway
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/3-landlord-gateway.yaml | kubectl delete -f -
  #PVC
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/2-landlord-PVC.yaml | sed "s/<storageclass>/${STORAGE_CLASS}/g" | kubectl delete -f -
  #service
  curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Landlord/1-landlord-service.yaml | kubectl delete -f -
  #configmap & namespace
  kubectl delete -n ${LANDLORD_NS} configmap kuser-conf
  kubectl delete ns ${LANDLORD_NS}
;;

images) #Check cluster images.
  [ -z ${2} ] && node-message ${@} && exit
  node-selector ${@}
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -en "${YELLOW}${install_list} | images list${NC} | quantity: "
      ssh ${install_list} sudo podman images | grep -v 'REPOSITORY' | wc -l
      ssh ${install_list} sudo podman images
    done
;;

image-send) #save >> scp >> load target image to every worker node [ <image-name> <name.tar> ]
  image=$(echo ${2} | cut -d ":" -f 1)
  sudo podman images | grep "${image}"
  [ ${?} != 0 ] && sudo podman pull ${2} || echo "Image is already exists."
  sudo podman save ${2} > ~/${3} 2> /dev/null
  for install_list in ${ALL_NODES_EX_LOCALHOST}
    do
      scp ~/${3} ${install_list}:
      ssh ${install_list} "sudo podman rmi ${2} 2> /dev/null"
      ssh ${install_list} "sudo podman load < ~/${3} 2> /dev/null"
      ssh ${install_list} "rm ~/${3}"
    done
  rm ./${3}
;;

image-rm) #Remove dangling images on cluster.
  [ -z ${2} ] && node-message ${@} && exit
  node-selector ${@}
  interrupt ${RED}Please confirm this command will delete unuse images on node:${YELLOW} ${cp_nodes} ${wk_nodes}${RED}!${NC}
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      ssh ${install_list} 'sudo podman image prune -f'
      ssh ${install_list} 'sudo podman rmi -a &> /dev/null'
      echo -en "${YELLOW}${install_list} | in-use images list${NC} | quantity: "
      ssh ${install_list} sudo podman images | grep -v 'REPOSITORY' | wc -l
      ssh ${install_list} sudo podman images
    done
;;

helm-repo) #Check helm repository.
  [ -z ${2} ] && echo -e "${RED}Please input parameter after ${YELLOW}\"helm-repo\" \n > [ add | update | check ]${NC}" && exit
  node-selector hosts
  [ "${2}" == "add" ] && helm-repo-add
  [ "${2}" == "update" ] && helm-repo-update
  [ "${2}" == "check" ] && helm-repo-check
;;

cluster-info) #Check kubernetes cluster info.
  if [ "${2}" == "cidr" ]
    then
      check-cidr
  elif [ "${2}" == "taints" ]
    then
      check-tains
  elif [ "${2}" == "etcd" ]
    then
      check-etcd-member
  elif [ "${2}" == "all" ]
    then
      kubectl get node > /dev/null 2>&1
      [ ${?} != 0 ] && echo -e " [${RED}●${NC}] This node not join cluster" && exit
      check-cidr
      check-tains
      check-etcd-member
    else
      echo -e "${RED}Please input parameter after \"taints\" \n ${YELLOW}> [ cidr | taints | etcd | all ]${NC}"
  fi
;;

cluster-upgrade) #Upgrade cluster [ upgrade kubeadm、kubectl、kubelet、crio ]
  [ -z ${2} ] && node-selector hosts || node-selector ${@}

  #Exclude non-join nodes
  exclude-non-join

  interrupt ${RED}Please confirm this command will upgrade nodes to ${KUBE_INIT_VER}: ${YELLOW}${cp_nodes} ${wk_nodes}!${NC}

  first_control_plane=`hostname`
  other_control_plane=$(echo "${cp_nodes} ${wk_nodes}" | tr -s ' ' '\n' | sed "/`hostname`/d" | grep '\-m' | tr -s '\n' ' ')
  CURRENT_VER=$(kubectl get nodes | grep `hostname` | awk '{ print $ 5}')

  for checklist in ${first_control_plane} ${other_control_plane} ${wk_nodes}
    do
      CHECK_VER=$(kubectl get nodes | grep ${checklist} | awk '{ print $ 5}')
      echo "${CHECK_VER}" | grep ${KUBE_INIT_VER} &> /dev/null
      [ ${?} == 0 ] && status=0 || status=1
      [ "${status}" == "0" ] && echo -e "${YELLOW}${checklist} version is already been ${RED}${KUBE_INIT_VER}${NC}"
      [ "${status}" == "1" ] && echo -e "${GREEN}${checklist} Upgrade version checked: ${RED}${CHECK_VER} >> ${KUBE_INIT_VER}${NC}"
    done
  [ "${status}" == "0" ] && echo && exit

  #[ -z ${2} ] && kdm pkg-repo add hosts || kdm pkg-repo add ${cp_nodes} ${wk_nodes}
  kdm pkg-repo add ${cp_nodes} ${wk_nodes}
  host-os-detection

  interrupt ${RED}start upgrade procedure?${NC}

  for FCP in ${first_control_plane}
    do
      if [ "${KUBE_INIT_VER}" == "${CURRENT_VER}" ]
        then
          echo -e "${YELLOW}Cluster upgrade procedure${NC}"
          echo ${cp_nodes} ${wk_nodes} | grep `hostname` &> /dev/null
          [ ${?} == 0 ] && echo -e "${YELLOW} - ${first_control_plane} version is already been ${RED}${KUBE_INIT_VER}${NC}"
          continue
      fi
      echo -e "${YELLOW}${FCP} | upgrade procedure${NC}"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf versionlock delete kubeadm"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf install -y kubeadm-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf versionlock add kubeadm"

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf versionlock delete kubeadm"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf install -y kubeadm-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf versionlock add kubeadm"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt-mark unhold kubeadm"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt install -qy kubeadm=${KUBE_SUBVER}-*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt-mark hold kubeadm"

      ssh ${FCP} "sudo kubeadm upgrade plan ${KUBE_INIT_VER}"
      ssh ${FCP} "sudo kubeadm upgrade apply --force ${KUBE_INIT_VER}" #sudo kubeadm upgrade node

      kubectl drain ${FCP} --ignore-daemonsets

      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf versionlock delete kubelet kubectl"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf install -y kubelet-${KUBE_SUBVER} kubectl-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf versionlock add kubelet kubectl"

    status=0
    CRI_PROVIDES=$(sudo dnf provides cri-o 2> /dev/null | grep 'Provide' | head -n 5 | cut -d '=' -f 2 | grep ${CRIO_RELEASE} | head -n 1)

      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf versionlock delete cri-o"
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${FCP} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${FCP} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${FCP} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${FCP} "sudo dnf versionlock add cri-o"

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf versionlock delete kubelet kubectl"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf install -y kubelet-${KUBE_SUBVER} kubectl-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf versionlock add kubelet kubectl"

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf versionlock delete cri-o"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${FCP} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${FCP} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${FCP} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${FCP} "sudo dnf versionlock add cri-o"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt-mark unhold kubelet kubectl"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt install -qy -o Dpkg::Options::="--force-confold" kubelet=${KUBE_SUBVER}-* kubectl=${KUBE_SUBVER}-*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt-mark hold kubelet kubectl"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt-mark unhold cri-o"&> /dev/null
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt install -qy -o Dpkg::Options::="--force-confold" cri-o=${CRIO_SUBVER}~*" &> /dev/null && status=1
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${FCP} "sudo apt-mark hold" &> /dev/null

      ssh ${FCP} "sudo systemctl daemon-reload"
      ssh ${FCP} "sudo systemctl restart --now crio"
      ssh ${FCP} "sudo systemctl restart --now kubelet"
      
      kubectl uncordon ${FCP}
    done

  for OCP in ${other_control_plane}
    do
      OCP_VER=$(kubectl get nodes | grep "${OCP}" | awk '{ print $ 5}')
      if [ "${KUBE_INIT_VER}" == "${OCP_VER}" ]
        then
          echo -e "${YELLOW} - ${OCP} version is already been ${RED}${KUBE_INIT_VER}${NC}"
          continue
      fi
      echo -e "${YELLOW}${OCP} | upgrade procedure${NC}"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf versionlock delete kubeadm"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf install -y kubeadm-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf versionlock add kubeadm"

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf versionlock delete kubeadm"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf install -y kubeadm-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf versionlock add kubeadm"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt update"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt-mark unhold kubeadm"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt install -qy kubeadm=${KUBE_SUBVER}-*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt-mark hold kubeadm"

      ssh ${OCP} "sudo kubeadm upgrade plan ${KUBE_INIT_VER}"
      ssh ${OCP} "sudo kubeadm upgrade node"

      kubectl drain ${OCP} --ignore-daemonsets

      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf versionlock delete kubelet kubectl"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf install -y kubelet-${KUBE_SUBVER} kubectl-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf versionlock add kubelet kubectl"
      
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf versionlock delete cri-o"
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${OCP} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${OCP} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${OCP} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${OCP} "sudo dnf versionlock add cri-o"

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf versionlock delete kubelet kubectl"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf install -y kubelet-${KUBE_SUBVER} kubectl-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf versionlock add kubelet kubectl"
      
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf versionlock delete cri-o"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${OCP} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${OCP} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${OCP} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${OCP} "sudo dnf versionlock add cri-o"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt-mark unhold kubelet kubectl"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt install -qy -o Dpkg::Options::="--force-confold" kubelet=${KUBE_SUBVER}-* kubectl=${KUBE_SUBVER}-*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt-mark hold kubelet kubectl"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt-mark unhold cri-o"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt install -qy -o Dpkg::Options::="--force-confold" cri-o=${CRIO_SUBVER}~*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${OCP} "sudo apt-mark hold cri-o"

      ssh ${OCP} "sudo systemctl daemon-reload"
      ssh ${OCP} "sudo systemctl restart --now crio"
      ssh ${OCP} "sudo systemctl restart --now kubelet"
      
      kubectl uncordon ${OCP}
    done

  for wlist in ${wk_nodes}
    do
      WORKER_VER=$(kubectl get nodes | grep "${wlist}" | awk '{ print $ 5}')
      if [ "${KUBE_INIT_VER}" == "${WORKER_VER}" ]
        then
          echo -e "${YELLOW} - ${wlist} version is already been ${RED}${KUBE_INIT_VER}${NC}"
          continue
      fi
      echo -e "${YELLOW}${wlist} | upgrade procedure${NC}"

      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf versionlock delete kubeadm"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf install -y kubeadm-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf versionlock add kubeadm"

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf versionlock delete kubeadm"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf install -y kubeadm-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf versionlock add kubeadm"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt update"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt-mark unhold kubeadm"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt install -qy kubeadm=${KUBE_SUBVER}-*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt-mark hold kubeadm"
      ssh ${wlist} "sudo kubeadm upgrade node"

      kubectl drain ${wlist} --ignore-daemonsets

      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf versionlock delete kubelet kubectl"
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf install -y kubelet-${KUBE_SUBVER} kubectl-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf versionlock add kubelet kubectl"

      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf versionlock delete cri-o"
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${wlist} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${wlist} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${wlist} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${wlist} "sudo dnf versionlock add cri-o"

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf versionlock delete kubelet kubectl"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf install -y kubelet-${KUBE_SUBVER} kubectl-${KUBE_SUBVER} --disableexcludes=kubernetes" 2> /dev/null
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf versionlock add kubelet kubectl"
      
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf versionlock delete cri-o"
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${wlist} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${wlist} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${wlist} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${wlist} "sudo dnf versionlock add cri-o"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt-mark unhold kubelet kubectl"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt install -qy -o Dpkg::Options::="--force-confold" kubelet=${KUBE_SUBVER}-* kubectl=${KUBE_SUBVER}-*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt-mark hold kubelet kubectl"

      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt-mark unhold cri-o"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt install -qy -o Dpkg::Options::="--force-confold" cri-o=${CRIO_SUBVER}~*"
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${wlist} "sudo apt-mark hold cri-o"

      ssh ${wlist} "sudo systemctl daemon-reload"
      ssh ${wlist} "sudo systemctl restart --now crio"
      ssh ${wlist} "sudo systemctl restart --now kubelet"

      kubectl uncordon ${wlist}
    done
    kubectl get nodes | tail -n +2 | awk '{ print $5 }' | grep ${KUBE_INIT_VER} &> /dev/null
    [ ${?} == 0 ] && echo -e "${YELLOW}Cluster has been upgraded to ${RED}${KUBE_INIT_VER}${NC}" || echo -e "${RED}Cluster has not upgrade to ${RED}${KUBE_INIT_VER}${NC}"
;;

cluster-rm) #Remove kubernetes cluster.
  node-selector hosts
  #Setup password
  setup-password-control

  #Rook-ceph detection
  kubectl get storageclass -A 2> /dev/null | grep 'rook-ceph' &> /dev/null
  [ ${?} == 0 ] && rook_detection=1

  #local-path detection
  kubectl get storageclass -A 2> /dev/null | grep 'local-path' &> /dev/null
  [ ${?} == 0 ] && local_path_detection=1

  #metrics detection
  kubectl get po -A | grep 'metrics' &> /dev/null
  [ ${?} == 0 ] && metrics_detection=1
    
  #Prometheus detection
  kubectl get ns -A 2> /dev/null | grep 'monitoring' &> /dev/null
  [ ${?} == 0 ] && prometheus_detection=1

  #elastic detection
  kubectl get ns -A 2> /dev/null | grep 'elastic-system' &> /dev/null
  [ ${?} == 0 ] && elastic_detection=1

  #calico detection
  kubectl get ns -A 2> /dev/null | grep 'calico' &> /dev/null
  [ ${?} == 0 ] && calico_detection=1

  #cluster remove info
  start-info
  echo -e "${YELLOW}StorageClass: $(kubectl get storageclass -A 2> /dev/null | grep -E 'rook-ceph|local-path' | awk '{ print $1 }' | sed ":a;N;s/\n/\ | /g")${NC}"
  interrupt ${RED}Please confirm this command will destruction cluster!${NC}
  echo -n "Processing"; echo -n "."; sleep 0.5; echo -n "."; sleep 0.5; echo "."; sleep 0.5

  #Check kubernetes status
  status=0
  kubectl get nodes &> /dev/null
  [ ${?} == 0 ] && status=1

  #Rook-ceph
  [ "${rook_detection}" == "1" ] && rook_obj_list=$(kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph | grep -v '^[0-9]' | grep -v '^[A-Z]' | awk '{ print $1 }')
  [ "${rook_detection}" == "1" ] && for list in ${rook_obj_list}
    do
      kubectl -n ${ROOK_CEPH_NS} patch ${list} -p '{"metadata":{"finalizers": []}}' --type=merge
    done

  #Patch all object
  all_obj_list=$(kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -A | grep -v '^[0-9]' | grep -v '^[A-Z]' | awk '{ print $1,$2 }' | grep -v 'rook-ceph' | grep '/' | sed 's/ /|/g')
  for list in ${all_obj_list}
    do
      obj_ns=$(echo ${list} | cut -d '|' -f 1)
      obj_name=$(echo ${list} | cut -d '|' -f 2)
      kubectl -n ${obj_ns} patch ${obj_name} -p '{"metadata":{"finalizers": []}}' --type=merge
    done

  #local-path
  [ "${local_path_detection}" == "1" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Local-path/local-path-storage.yaml | sed "s/<image-version>/${LOCAL_PATH_VER}/g" | kubectl delete -f -
  [ ${?} == 0 ] && [ "${local_path_detection}" == "1" ] && echo "local-path-storage deleted."

  #Metrics
  [ "${metrics_detection}" == "1" ] && [ "${status}" == "1" ] && helm delete metrics-server -n kube-system
  [ "${metrics_detection}" == "1" ] && [ "${status}" == "1" ] && helm repo remove metrics-server
  
  #Prometheus
  [ "${prometheus_detection}" == "1" ] && [ "${status}" == "1" ] && helm delete kube-prometheus-stack -n monitoring
  [ "${prometheus_detection}" == "1" ] && [ "${status}" == "1" ] && helm repo remove prometheus-community

  #Delete Kibana & Ingress rule
  [ "${elastic_detection}" == "1" ] && [ "${status}" == "1" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/kibana-ingress.yaml | kubectl delete -f -
  [ "${elastic_detection}" == "1" ] && [ "${status}" == "1" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/kibana.yaml | sed "s/<eck-version>/${ECK_VER}/g" | kubectl delete -f -
  
  #Delete elasticsearch
  [ "${elastic_detection}" == "1" ] && [ "${status}" == "1" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/elasticsearch.yaml | sed "s/<eck-version>/${ECK_VER}/g" | sed 's/<storageClassName>/rook-ceph-block/g' | kubectl delete -f -
  #Delete Superuser
  [ "${elastic_detection}" == "1" ] && [ "${status}" == "1" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/ECK/filerealm-secret.yaml | sed 's/<user>/bigred/g' | sed "s/<password>/${ECK_PASSWORD}/g" | kubectl delete -f -
  #Delete crd & operator
  [ "${elastic_detection}" == "1" ] && [ "${status}" == "1" ] && kubectl delete -f https://download.elastic.co/downloads/eck/${ECK_OP_VER}/operator.yaml
  [ "${elastic_detection}" == "1" ] && [ "${status}" == "1" ] && kubectl delete -f https://download.elastic.co/downloads/eck/${ECK_OP_VER}/crds.yaml
  
  #calico
  [ "${calico_detection}" == "1" ] && [ "${status}" == "1" ] && helm delete calico -n tigera-operator
  [ "${calico_detection}" == "1" ] && [ "${status}" == "1" ] && kubectl delete namespace tigera-operator
  [ "${calico_detection}" == "1" ] && [ "${status}" == "1" ] && helm repo remove projectcalico

  #delete all namespace
  [ "${status}" == "1" ] && kubectl delete all --all --all-namespaces 2> /dev/null
  [ "${status}" == "1" ] && kubectl delete --all namespaces 2> /dev/null

  for wclist in ${wk_nodes} ${cp_nodes}
    do
      echo -e "${YELLOW}${wclist} | delete process${NC}"
      ssh ${wclist} 'sudo kubeadm reset -f &> /dev/null'
      [ ${?} == 0 ] && echo -e " [${GREEN}●${NC}] kubeadm reset successed" || echo -e " [${RED}●${NC}] kubeadm reset not success"
      dir-delete-list
      ssh ${wclist} "sudo systemctl restart --now crio"
      ssh ${wclist} "sudo systemctl restart --now kubelet"
      nc -z -w 1 ${IP} 10250 > /dev/null 2>&1
      [ ${?} == 0 ] && kubectl delete node ${wclist} &> /dev/null
    done
  [ "${rook_detection}" == "1" ] && rook-wipe-data hosts
;;

cri-upgrade) #Update crio package.
  node-selector ${@}
  interrupt ${RED}Please confirm this command will let${YELLOW} ${cp_nodes} ${wk_nodes} ${RED}cri-o upgrade!${NC}

  for install_list in ${cp_nodes} ${wk_nodes}
    do
      kubectl drain ${install_list} --ignore-daemonsets 2> /dev/null
      #install & setup cri-o
      echo -e "\n${YELLOW}${install_list} | package upgrade procedure${NC}"
      #Add crio、kubernetes package repositories
      package-repository-add

      status=0
      CRI_PROVIDES=$(sudo dnf provides cri-o 2> /dev/null | grep 'Provide' | head -n 5 | cut -d '=' -f 2 | grep ${CRIO_RELEASE} | head -n 1)

      [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "sudo dnf versionlock delete cri-o" &> /dev/null
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${install_list} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${install_list} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "RHEL_8" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${install_list} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "RHEL_8" ] && ssh ${install_list} "sudo dnf versionlock add cri-o" &> /dev/null

      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "sudo dnf versionlock delete cri-o" &> /dev/null
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "sub" ] && ssh ${install_list} "sudo dnf install -y cri-o-${CRIO_SUBVER}-* crun" > /dev/null 2>&1 && status=1
      [ "${status}" == "0" ] && ssh ${install_list} "sudo dnf install -y cri-o-${CRI_PROVIDES} crun" > /dev/null 2>&1 && status=1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && [ "${VER_CONTROL}" == "latest" ] && ssh ${install_list} "sudo dnf install -y cri-o-${CRIO_RELEASE}.* crun" > /dev/null 2>&1
      [ "${OS_VER}" == "CentOS_8_Stream" ] && ssh ${install_list} "sudo dnf versionlock add cri-o" &> /dev/null
      
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "sudo apt-mark unhold cri-o" &> /dev/null
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "sudo apt install -qy -o Dpkg::Options::="--force-confold" cri-o=${CRIO_SUBVER}~*" &> /dev/null && status=1
      [ "${OS_VER}" == "xUbuntu_22.04" ] && ssh ${install_list} "sudo apt-mark hold" &> /dev/null

      echo "`crio-install-check ${install_list}`"
      ssh ${install_list} "sudo systemctl daemon-reload" && echo
      kubectl uncordon ${install_list}
    done
;;

cri-check) #Check CRI running pods.
  [ -z ${2} ] && node-message ${@} && exit
  node-selector ${@}
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}${install_list} | check list${NC}"
      ssh ${install_list} "sudo crictl ps -a"
    done
;;

cri-rm) #Remove CRI running pods.
  [ -z ${2} ] && node-message ${@} && exit
  node-selector ${@}
  interrupt ${RED}Please confirm this command will delete all container via crio on node:${YELLOW} ${cp_nodes} ${wk_nodes}${NC}
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo "${install_list} | check list"
      cri_list=$(ssh ${install_list} sudo crictl ps -a | tail -n +2 | awk '{ print $9 }' | tr -s '\n' ' ')
      for list in ${cri_list}
        do
          ssh ${install_list} "sudo crictl stopp ${list}" &> /dev/null
          ssh ${install_list} "sudo crictl rmp ${list}" &> /dev/null
        done
      ssh ${install_list} "sudo crictl ps -a"
      echo
    done
;;

node-info) #Show host basic information.
  node-selector hosts
  for cwlist in ${cp_nodes} ${wk_nodes}
    do
        echo "Node name: ${cwlist}"
        #cpu information
        cpu_name=$(ssh ${cwlist} sudo cat /proc/cpuinfo | grep 'model name' | head -n 1 | cut -d ':' -f2 | tr -s '-' ' ' | sed 's/(R)//g; s/(TM)//g; s/@ //g' | sed 's/^.//')
        core_number=$(ssh ${cwlist} sudo cat /proc/cpuinfo | grep 'model name' | wc -l)
        cpu_architecture=$(ssh ${cwlist} uname -m)
        echo -e " CPU: ${YELLOW}$cpu_name (core: $core_number) | ${cpu_architecture}${NC}"
        #memory information
        ssh ${cwlist} sudo free -mh | grep Mem: | awk '{ print $2 }' | grep 'Gi' &> /dev/null
        [ ${?} == 0 ] && byte=GB || byte=MB
        m_size=$(ssh ${cwlist} sudo free -mh | grep Mem: | awk '{ print $2 }' | sed 's/Gi//g')
        Gi_to_GB=$(awk "BEGIN { print $m_size / .93 }")
        echo -e " Memory: ${YELLOW}${Gi_to_GB} ${byte}${NC}"
    done
;;

node-check) #Check nodes port | hostname. [ hosts | hosts name | hosts port | NETID start end <Port> ]
  while [ 1 ]
    do
      clear
      node-check $@ && sleep 3
    done
;;

node-reset) #Reset hosts | specify nodes. [ node-reset node ]
  [ -z ${2} ] && node-message ${@} && exit
  node-selector ${@}
  interrupt ${RED}Please confirm this command will destruction node:${YELLOW} ${cp_nodes} ${wk_nodes}${RED}!${NC}

  echo -n "Processing"; echo -n "."; sleep 0.5; echo -n "."; sleep 0.5; echo "."; sleep 0.5

  for clist in ${cp_nodes}
    do
      status=0
      etcd_pod=$(kubectl get pods -n kube-system 2> /dev/null | grep 'etcd' | awk '{ print $1 }' | head -n 1)
      etcd_endpoints=$(echo ${etcd_pod} | cut -d '-' -f 2)
      echo -e "${YELLOW}${clist} | delete process${NC}"
      kubectl get nodes 2> /dev/null | grep "${clist}" &> /dev/null
      [ ${?} == 0 ] && kubectl delete node ${clist} && status=1
      [ "${status}" == "1" ] && remove_id=$(kubectl exec -it ${etcd_pod} -n kube-system -- etcdctl --endpoints ${NETID}.${etcd_endpoints}:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt member list | grep "${clist}" | cut -d ',' -f 1 2> /dev/null)
      [ "${status}" == "1" ] && kubectl exec -it ${etcd_pod} -n kube-system -- etcdctl --endpoints ${NETID}.${etcd_endpoints}:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt member remove ${remove_id} 2> /dev/null
      nc -z -w 1 ${clist} 10250 > /dev/null 2>&1
      [ ${?} == 0 ] && ssh ${clist} 'sudo kubeadm reset -f'
      dir-delete-list

      ssh ${clist} "sudo systemctl enable --now crio"
      ssh ${clist} "sudo systemctl enable --now kubelet"
    done

  for wlist in ${wk_nodes}
    do
      status=0
      echo -e "${YELLOW}${wlist} | delete process${NC}"
      nc -z -w 1 ${wlist} 10250 > /dev/null 2>&1
      [ ${?} == 0 ] && kubectl delete node ${wlist} && status=1
      nc -z -w 1 ${wlist} 10250 > /dev/null 2>&1
      [ ${?} == 0 ] && ssh ${wlist} 'sudo kubeadm reset -f'
      dir-delete-list

      ssh ${wlist} "sudo systemctl enable --now crio"
      ssh ${wlist} "sudo systemctl enable --now kubelet"
    done

  node-power reboot ${@}
;;

node-power) #Reboot/Poweroff hosts | specify node. [ node ]
  node-power ${@}
;;

deploy) #Automatic deploy kubernetes cluster. [ kdm deploy high calico/flannel hosts 100 109 local-path/rook-ceph ]
  #kdm deploy high calico hosts 100 109 rook-ceph
  [ "${2}" == "example" ] && echo -e "${YELLOW}high calico hosts 100 109 rook-ceph" && exit
  [ ${#} != "7" ] && echo -e "Input parameter ${YELLOW}[ deploy | high | calico/flannel | hosts | 100 | 109 | local-path/rook-ceph ]${NC}" && exit
  kdm cp-init ${2} ${3}
  [ "${2}" == "high" ] && kdm cp-join ${4}
  kdm wk-join ${4}
  kdm dns-rollout
  kdm controller-deploy ${5} ${6}
  kdm metrics-deploy
  kdm prometheus-deploy
  kdm csi-deploy ${7}
;;

etcdctl-install)
  #Variable
  ETCD_VER=v3.5.7
  ETCD_DIR=etcd-download
  DOWNLOAD_URL=https://github.com/coreos/etcd/releases/download
  #Download etcdctl
  mkdir ${ETCD_DIR}
  cd ${ETCD_DIR}
  wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz
  tar -xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz
  #install etcdctl
  cd etcd-${ETCD_VER}-linux-amd64
  sudo cp etcdctl /usr/local/bin/
  source /etc/profile
;;

help) #Show script parameters information.
  ls ~/kdm/kdm &> /dev/null
  [ ${?} == 0 ] && stage=0
  ls ~/bin/kdm &> /dev/null
  [ ${?} == 0 ] && stage=1
  [ ${stage} == 0 ] && path="kdm/kdm"
  [ ${stage} == 1 ] && path="bin/kdm"

  #System setup
  list=" > sys-| > set-| > sync-| > pkg-| > k9s-| > daemon- | > docker-"
  number=$(cat ~/${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)/:/g' | sed 's/^/ > /' | sed 's/#//g' | grep -E "${list}" | wc -l)
  echo -e "system-setup | ${YELLOW}${number}${NC}"
  cat ~/${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)/:/g' | sed 's/^/ > /' | sed 's/#//g' | grep -E "${list}"
  echo

  #Kubernetes deploy
  list=" > cp-| > wk-| > cni-| > csi-| > dns-"
  number=$(cat ~/${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)/:/g' | sed 's/^/ > /' | sed 's/#//g' | grep -E "${list}" | wc -l)
  echo -e "Kubernetes-deploy | ${YELLOW}${number}${NC}"
  echo "  └─cp-init >> cp-join >> wk-join >> dns-rollout >> controller-deploy >> metrics-deploy >> prometheus-deploy >> csi-deploy"
  cat ~/${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)/:/g' | sed 's/^/ > /' | sed 's/#//g' | grep -E "${list}"
  echo

  #Kubernetes functions
  list=" > nodes:| > node-| > pods:| > images:| > image-| > helm-| > cluster-| > cri-| > help:"
  number=$(cat ~/${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)/:/g' | sed 's/^/ > /' | sed 's/#//g' | grep -E "${list}" | wc -l)
  echo -e "Kubernetes-functions | ${YELLOW}$((${number}+2))${NC}"
  service_list=$(cat ~/${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)//g' | grep '\-deploy' | grep -vE 'cni|csi' | sed 's/-deploy//g' | tr -s '#' '\n' | grep -v '^D' | sed ":a;N;s/\n/| /g;ta")
  echo -e " > item-deploy: Deploy Kubenetes projects.\n  └─ [ ${service_list} ]"
  echo -e " > item-rm: Delete Kubenetes projects.\n  └─ [ ${service_list} ]"
  cat ~/${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)/:/g' | sed 's/^/ > /' | sed 's/#//g' | grep -E "${list}"
;;

parm-check) #List all script parameter.
  ls ~/kdm/kdm &> /dev/null
  [ ${?} == 0 ] && stage=0
  ls ~/bin/kdm &> /dev/null
  [ ${?} == 0 ] && stage=1
  [ ${stage} == 0 ] && path="kdm/kdm"
  [ ${stage} == 1 ] && path="bin/kdm"
  echo -e "${YELLOW}Parameters check list${NC}"
  cat ${path} | grep "[a-z])" | sed '/\$/d;/echo/d' | sed 's/)/:/g' | sed 's/^/ > /' | sed 's/#//g'
;;

\-) #Check kubernetes objects.
  shift
  nc -z -w 1 ${IP} 10250 > /dev/null 2>&1
  [ ${?} == 0 ] && k9s -c ${@} 2> /dev/null || echo -e " [${YELLOW}●${NC}] This node not install k9s." || echo -e " [${YELLOW}●${NC}] This node not activate."
;;

\-\-)
  shift
  node-selector ${@}
  shift
  for install_list in ${cp_nodes} ${wk_nodes}
    do
      echo -e "${YELLOW}${install_list} | Result${NC}"
      ssh -o ConnectTimeout=1 -o BatchMode=yes ${install_list} 'hostname' &> /dev/null
      [ ${?} != 0 ] && echo -e "${RED}This node not activate${NC}"
      ssh -o ConnectTimeout=1 ${install_list} "${@}" 2> /dev/null
      [ ${?} != 0 ] && echo -e "${RED}Failed${NC}"
      echo
    done
;;

test-nginx)
  [ -z ${2} ] && echo -e "Input parameter ${YELLOW}[ deploy | remove ]${NC}" && exit
  set-environment-variable
  [ "${2}" == "deploy" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Test/test-nginx.yaml | sed "s/<storageClassName>/${STORAGE_CLASS}/g" | sed "s/<ingressClass>/${INGRESS_CLASS_NAME}/g" | kubectl apply -f -
  [ "${2}" == "remove" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Test/test-nginx.yaml | sed "s/<storageClassName>/${STORAGE_CLASS}/g" | sed "s/<ingressClass>/${INGRESS_CLASS_NAME}/g" | kubectl delete -f -
;;

test-apache)
  [ -z ${2} ] && echo -e "Input parameter ${YELLOW}[ deploy | remove ]${NC}" && exit
  set-environment-variable
  [ "${2}" == "deploy" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Test/test-apache.yaml | sed "s/<storageClassName>/${STORAGE_CLASS}/g" | sed "s/<ingressClass>/${INGRESS_CLASS_NAME}/g" | kubectl apply -f -
  [ "${2}" == "remove" ] && curl -s https://raw.githubusercontent.com/Bookman-W/Kubernetes-yaml/main/Test/test-apache.yaml | sed "s/<storageClassName>/${STORAGE_CLASS}/g" | sed "s/<ingressClass>/${INGRESS_CLASS_NAME}/g" | kubectl delete -f -
;;

test-images-push)
  #Pull all system images
  #sudo kubeadm config images pull --image-repository=registry.k8s.io --kubernetes-version=1.26.3
  #上述指令要解決 pull x509 憑證問題
  harbor_domain=registry.vattenlab.com
  harbor_project_1=kube-system
  target_image_list_1=$(sudo podman images --sort repository --format "table {{.Repository}} {{.Tag}}" | tail -n +2 | grep 'registry.k8s.io' | tr -s ' ' ':')
  result_image_list_1=$(sudo podman images --sort repository --format "table {{.Repository}} {{.Tag}}" | tail -n +2 | grep 'registry.k8s.io' | tr -s ' ' ':' | sed 's/registry.k8s.io\///g')
  count=0
  for target_list in ${target_image_list_1}
    do
      echo Target: ${target_list}
      result_list=$(echo ${target_list} | sed 's/registry.k8s.io\///g')
      sudo podman tag ${target_list} ${harbor_domain}/${harbor_project_1}/${result_list}
      echo Result: ${result_list}
      sudo podman push ${harbor_domain}/${harbor_project_1}/${result_list} --tls-verify=false
    done

  harbor_project_2=calico
  target_image_list_2=$(sudo podman images --sort repository --format "table {{.Repository}} {{.Tag}}" | tail -n +2 | grep 'docker.io/calico/' | tr -s ' ' ':')
  result_image_list_2=$(sudo podman images --sort repository --format "table {{.Repository}} {{.Tag}}" | tail -n +2 | grep 'docker.io/calico/' | tr -s ' ' ':' | sed 's/docker.io\/calico\///g')
  for target_list in ${target_image_list_2}
    do
      echo Target: ${target_list}
      result_list=$(echo ${target_list} | sed 's/docker.io\/calico\///g')
      sudo podman tag ${target_list} ${harbor_domain}/${harbor_project_2}/${result_list}
      echo Result: ${result_list}
      sudo podman push ${harbor_domain}/${harbor_project_2}/${result_list} --tls-verify=false
    done
;;

test-images-rm)
  #Pull all system images
  #sudo kubeadm config images pull --image-repository=registry.k8s.io --kubernetes-version=1.26.3
  #上述指令要解決 pull x509 憑證問題
  harbor_domain=registry.vattenlab.com
  target_image_list_1=$(sudo podman images --sort repository --format "table {{.Repository}} {{.Tag}}" | tail -n +2 | grep 'registry.vattenlab.com' | tr -s ' ' ':')
  count=0
  for target_list in ${target_image_list_1}
    do
      sudo podman rmi ${target_list}
    done
;;

test)
  rook-wipe-data hosts
;;

"")
  start-info
;;

*)
  echo -e "${YELLOW} \"${@}\" ${NC}is not effective parameter!"
;;

esac